{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dce818ed",
   "metadata": {},
   "source": [
    "# Домашнее задание 2 \n",
    "## Contrastive and non-contrastive methods in CXR images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc72c49b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Оценивание"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23244fa8-f03c-4e81-92be-fb54a9a20b26",
   "metadata": {},
   "source": [
    "Задание должно быть выполнено самостоятельно. Похожие решения будут считаться плагиатом. Если вы опирались на внешний источник в реализации, необходимо указать ссылку на него. \n",
    "\n",
    "В качестве решения, необходимо предоставить код (`train.py` с аргументами для выбора датасета/метода) + отчет, в котором будут отображены все детали выбора гиперпараметров, комментарии, сопровождающие графики, а так же ответы на вопросы в ДЗ. Оформляйте отчет четко и читаемо. Плохо оформленный код, плохо оформленные графики негативно скажутся на оценке, так же как и неэффективная реализация."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56f10bb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Введение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded48ce1-b7b4-408c-bd9f-f25c12a22b9e",
   "metadata": {},
   "source": [
    "Вам предстоит реализовать (задание 0) и поработать с двумя методами - [SimCLR](https://arxiv.org/abs/2002.05709) и [VICReg](https://arxiv.org/abs/2105.04906). Обучать их будем на датасете, относящемся к домену медицинских изображений (задания 1-4). Подключим онлайн пробинг (задание 3), а так же сравним с трансфером с imagenet домена в этот мед домен (задание 4)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e111a04",
   "metadata": {},
   "source": [
    "### Датасеты [MedMNIST+](https://medmnist.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c31d27-9f38-463d-b8e7-41d815576a7b",
   "metadata": {},
   "source": [
    "Будем использовать уже подготовленные сконвертированные из DICOM'ов картинки. MedMNIST включает в себя два релевантных для нас датасета с рентгеновскими снимками грудной клетки:\n",
    "\n",
    "\n",
    "| MedMNIST2D     | Data Modality | Tasks (# Classes/Labels)           | # Samples | # Training | # Validation | # Test |\n",
    "|----------------|---------------|------------------------------------|-----------|------------|--------------|--------|\n",
    "| ChestMNIST     | Chest X-Ray   | Multi-Label (14), Binary-Class (2) | 112,120   | 78,468     | 11,219       | 22,433 |\n",
    "| PneumoniaMNIST | Chest X-Ray   | Binary-Class (2)                   | 5,856     | 4,708      | 524          | 624    |\n",
    "\n",
    "На этот раз будем использовать разрешение 224x224 (необходимо выставить `size` при инициализации датасета). Несколько картинок из ChestMNIST:\n",
    "\n",
    "![CXR image examples from ChestMNIST](data/cxr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808d9f49",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Задание 0 (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec42ac2-870b-4aa3-b15e-d6de1a2d6ab9",
   "metadata": {},
   "source": [
    "Реализуйте SimCLR и VICReg на базе ResNet-18 энкодера. Для этого надо реализовать соответствующие лосс-функции и архитектуры проекционных голов. Убедитесь, в корректности реализации на CIFAR-10 (не забудьте применить коррекцию резнета для картинок разрешением 32x32 из предыдущего домашнего задания). Для этого, сначала сделайте предобучение на train части датасета в течении 100 эпох, затем сделайте линейный пробинг с замороженным выучившимся энкодером.\n",
    "\n",
    "Референсный интервал top-1 accuracy для 100 эпох предобучения ~80-83% на линейном пробинге (если не получается, проверьте реализацию оптимизатора (**LARS**) и расписания шага обучения (`warmup_cosine`) или попробуйте подвигать learning rate).\n",
    "\n",
    "**NB**\n",
    "Чтобы сэкономить на психотерапевте, используйте оптимизатор [LARS](https://arxiv.org/abs/1708.03888) и `LinearWarmupCosineAnnealing` шедулер. Их нет в торче, но довольно просто реализовать самому или взять референсную реализацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a2e0e25-a7b3-4429-90c3-6343e9b917f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d381201d-9f8a-4d94-b082-0b25c903a69d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 170M/170M [00:46<00:00, 3.67MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 5.3763\n",
      "Epoch 2/100, Loss: 5.1443\n",
      "Epoch 3/100, Loss: 5.1092\n",
      "Epoch 4/100, Loss: 5.0813\n",
      "Epoch 5/100, Loss: 5.0561\n",
      "Epoch 6/100, Loss: 5.0277\n",
      "Epoch 7/100, Loss: 4.9996\n",
      "Epoch 8/100, Loss: 4.9709\n",
      "Epoch 9/100, Loss: 4.9426\n",
      "Epoch 10/100, Loss: 4.9165\n",
      "Epoch 11/100, Loss: 4.8947\n",
      "Epoch 12/100, Loss: 4.8723\n",
      "Epoch 13/100, Loss: 4.8490\n",
      "Epoch 14/100, Loss: 4.8310\n",
      "Epoch 15/100, Loss: 4.8129\n",
      "Epoch 16/100, Loss: 4.7960\n",
      "Epoch 17/100, Loss: 4.7817\n",
      "Epoch 18/100, Loss: 4.7700\n",
      "Epoch 19/100, Loss: 4.7592\n",
      "Epoch 20/100, Loss: 4.7475\n",
      "Epoch 21/100, Loss: 4.7393\n",
      "Epoch 22/100, Loss: 4.7301\n",
      "Epoch 23/100, Loss: 4.7213\n",
      "Epoch 24/100, Loss: 4.7156\n",
      "Epoch 25/100, Loss: 4.7105\n",
      "Epoch 26/100, Loss: 4.7060\n",
      "Epoch 27/100, Loss: 4.6992\n",
      "Epoch 28/100, Loss: 4.6948\n",
      "Epoch 29/100, Loss: 4.6907\n",
      "Epoch 30/100, Loss: 4.6859\n",
      "Epoch 31/100, Loss: 4.6828\n",
      "Epoch 32/100, Loss: 4.6757\n",
      "Epoch 33/100, Loss: 4.6735\n",
      "Epoch 34/100, Loss: 4.6689\n",
      "Epoch 35/100, Loss: 4.6627\n",
      "Epoch 36/100, Loss: 4.6628\n",
      "Epoch 37/100, Loss: 4.6598\n",
      "Epoch 38/100, Loss: 4.6571\n",
      "Epoch 39/100, Loss: 4.6531\n",
      "Epoch 40/100, Loss: 4.6506\n",
      "Epoch 41/100, Loss: 4.6488\n",
      "Epoch 42/100, Loss: 4.6452\n",
      "Epoch 43/100, Loss: 4.6410\n",
      "Epoch 44/100, Loss: 4.6400\n",
      "Epoch 45/100, Loss: 4.6372\n",
      "Epoch 46/100, Loss: 4.6385\n",
      "Epoch 47/100, Loss: 4.6319\n",
      "Epoch 48/100, Loss: 4.6338\n",
      "Epoch 49/100, Loss: 4.6310\n",
      "Epoch 50/100, Loss: 4.6266\n",
      "Epoch 51/100, Loss: 4.6269\n",
      "Epoch 52/100, Loss: 4.6238\n",
      "Epoch 53/100, Loss: 4.6221\n",
      "Epoch 54/100, Loss: 4.6214\n",
      "Epoch 55/100, Loss: 4.6176\n",
      "Epoch 56/100, Loss: 4.6186\n",
      "Epoch 57/100, Loss: 4.6147\n",
      "Epoch 58/100, Loss: 4.6145\n",
      "Epoch 59/100, Loss: 4.6117\n",
      "Epoch 60/100, Loss: 4.6115\n",
      "Epoch 61/100, Loss: 4.6120\n",
      "Epoch 62/100, Loss: 4.6080\n",
      "Epoch 63/100, Loss: 4.6068\n",
      "Epoch 64/100, Loss: 4.6073\n",
      "Epoch 65/100, Loss: 4.6070\n",
      "Epoch 66/100, Loss: 4.6040\n",
      "Epoch 67/100, Loss: 4.6016\n",
      "Epoch 68/100, Loss: 4.6024\n",
      "Epoch 69/100, Loss: 4.6005\n",
      "Epoch 70/100, Loss: 4.6013\n",
      "Epoch 71/100, Loss: 4.5975\n",
      "Epoch 72/100, Loss: 4.5980\n",
      "Epoch 73/100, Loss: 4.5968\n",
      "Epoch 74/100, Loss: 4.5966\n",
      "Epoch 75/100, Loss: 4.5946\n",
      "Epoch 76/100, Loss: 4.5927\n",
      "Epoch 77/100, Loss: 4.5939\n",
      "Epoch 78/100, Loss: 4.5909\n",
      "Epoch 79/100, Loss: 4.5905\n",
      "Epoch 80/100, Loss: 4.5929\n",
      "Epoch 81/100, Loss: 4.5906\n",
      "Epoch 82/100, Loss: 4.5892\n",
      "Epoch 83/100, Loss: 4.5897\n",
      "Epoch 84/100, Loss: 4.5872\n",
      "Epoch 85/100, Loss: 4.5864\n",
      "Epoch 86/100, Loss: 4.5887\n",
      "Epoch 87/100, Loss: 4.5866\n",
      "Epoch 88/100, Loss: 4.5868\n",
      "Epoch 89/100, Loss: 4.5860\n",
      "Epoch 90/100, Loss: 4.5859\n",
      "Epoch 91/100, Loss: 4.5859\n",
      "Epoch 92/100, Loss: 4.5848\n",
      "Epoch 93/100, Loss: 4.5857\n",
      "Epoch 94/100, Loss: 4.5870\n",
      "Epoch 95/100, Loss: 4.5863\n",
      "Epoch 96/100, Loss: 4.5835\n",
      "Epoch 97/100, Loss: 4.5840\n",
      "Epoch 98/100, Loss: 4.5830\n",
      "Epoch 99/100, Loss: 4.5834\n",
      "Epoch 100/100, Loss: 4.5844\n",
      "Epoch 10, Accuracy: 75.95%\n",
      "Epoch 20, Accuracy: 76.84%\n",
      "Epoch 30, Accuracy: 76.77%\n",
      "Epoch 40, Accuracy: 77.49%\n",
      "Epoch 50, Accuracy: 78.02%\n",
      "Epoch 60, Accuracy: 77.89%\n",
      "Epoch 70, Accuracy: 77.82%\n",
      "Epoch 80, Accuracy: 77.72%\n",
      "Epoch 90, Accuracy: 77.78%\n",
      "Epoch 100, Accuracy: 77.88%\n"
     ]
    }
   ],
   "source": [
    "base_dataset = datasets.CIFAR10(root='./data', train=True, download=True)\n",
    "ssl_dataset = SSLDataset(base_dataset, get_ssl_transforms())\n",
    "ssl_loader = DataLoader(ssl_dataset, batch_size=256, shuffle=True, num_workers=4)\n",
    "\n",
    "model = SimCLR()\n",
    "model = pretrain_ssl(model, ssl_loader, epochs=100)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=train_transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=4)\n",
    "\n",
    "linear_probe(model.encoder, train_loader, test_loader, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8803efb6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Задание 1 (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626b15b2-96e6-472b-aaad-0d9177813615",
   "metadata": {},
   "source": [
    "Загрузите упомянутые датасеты из `MedMNIST+` и проанализируйте данные. Например, посмотрите на количество и баланс классов, как устроена разметка по классам, найдите среднее и дисперсию значений пикселей. Определите **подходящие метрики и лосс** для конечной задачи для **каждого** из датасетов, аргументировано объясните ваш выбор."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96c6823-9168-4f90-a959-e320f45db781",
   "metadata": {},
   "source": [
    "Это задание выполнено в \"src/EDA.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaa5120",
   "metadata": {},
   "source": [
    "## Задание 2 (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35463063-b2f6-4e1e-aeb1-4ca1785e1cea",
   "metadata": {},
   "source": [
    "CXR изображения выглядят специфично. Кажется, что нужно иметь и специфичные для таких картинок аугментации.\n",
    "Поиграйтесь с трансформами и зафиксируйте набор, с которым вы будете проводить финальные запуски предобучения.\n",
    "\n",
    "### Каким образом можно определить подходящие аугментации?\n",
    "\n",
    "За неимением экспертного знания (если есть знакомый врач-рентгенолог, можно посоветоваться), будем отталкиваться от набора аугментаций в естественных картинках. Начнем с набора, используемого в SimCLR-подобных методах, для ImageNet. Примерно так готовый набор выглядит в `torchvision`'е (обратите внимание, что при создании `СolorJitter` указываются не сами интвервалы, а дельта, т.е. `brightness=0.4` дает `(0.6, 1.4)`):\n",
    "\n",
    "```python\n",
    "Compose(\n",
    "      RandomResizedCrop(\n",
    "          size=(224, 224),\n",
    "          scale=(0.08, 1.0),\n",
    "          ratio=(0.75, 1.3333333333333333),\n",
    "          interpolation=InterpolationMode.BICUBIC,\n",
    "          antialias=True)\n",
    "      RandomApply(\n",
    "          ColorJitter(\n",
    "              brightness=(0.6, 1.4),\n",
    "              contrast=(0.6, 1.4),\n",
    "              saturation=(0.8, 1.2),\n",
    "              hue=(-0.1, 0.1)))\n",
    "      RandomGrayscale(p=0.2)\n",
    "      GaussianBlur(p=0.5)\n",
    "      Solarization(p=0.1)\n",
    "      RandomHorizontalFlip(p=0.5)\n",
    "      ToTensor()\n",
    "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.228, 0.224, 0.225], inplace=False)\n",
    "```\n",
    "\n",
    "Какие параметры аугментаций стоило бы поменять? Реализуйте набор трансформов, посмотрите какие картинки получаются на выходе (не забудьте перевести выход в нужный интервал значений для визуализации), поиграйтесь со значениями параметров (например, `scale` в `RandomResizedCrop` или `brightness` в `ColorJitter`). Какие трансформы стоит убрать? Попробуйте добавить инвертирование и повороты картинок на небольшой угол."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f218776-5638-4726-9905-f8edf9c47367",
   "metadata": {},
   "source": [
    "Рентгеновский снимок - это не отражение света, а проекция плотности тканей, поэтому дефолтный набор аугментаций будет работать плохо. \n",
    "\n",
    "Остальной анализ и подбор аугментаций описан в src/EDA.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a36aef-2222-4908-8efd-0d3364ecee09",
   "metadata": {},
   "source": [
    "Для chestmnist у нас явно задача несбаллансированной классификации в обоих случаях. В качестве лосса просто возьмем BCE и навесим на меньший класс class weight множитель. В качестве метрики в обоих случаях возьмем PR-AUC. Также можно будет дополнительно померять F-beta score для какого-то порога, так как мы работаем с медицинскими данными, хотелось бы значимость рекола выкуртить и брать F2/F3 score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7ed726",
   "metadata": {},
   "source": [
    "## Задание 3 (4 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538d28e6-c567-45a3-b5b6-293630dd1cfb",
   "metadata": {},
   "source": [
    "Для того, чтобы честно найти подходящий набор аугментаций, надо проводить этап предобучения и затем оценивать качество получившихся репрезентаций на конечной задаче. Можно ли это как-то ускорить? Раз у нас есть разметка для всего датасета, воспользуемся ей и ускорим подбор аугментаций с помощью online probing'a.\n",
    "Для этого, добавим голову для линейного пробинга `linear_probe` к нашему энкодеру (`backbone`) и проекционной голове (`projection_head`). Эта линейная \"проба\" будет состоять из одного линейного слоя из размерности выхода энкодера (например, 512 для ResNet18) в число классов на конечной задаче (например, 14 классов у ChestMNIST)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881f35e8",
   "metadata": {},
   "source": [
    "На каждой итерации **предобучения** будем учить `backbone` и `projection_head` на претекстовую задачу (например, SimCLR лосс), а линейную пробу на классификацию.\n",
    "Получается такая двуглавая архитектура, где градиенты с претекстового лосса текут по проекционной голове и энкодеру, а градиенты с классификационного лосса только по линейной пробе (не забудьте правильно `detach'`нуться).\n",
    "\n",
    "```\n",
    "                      projection_head(h) -> ssl_loss\n",
    "                    /\n",
    "x -> encoder(x) -> h\n",
    "                    \\\n",
    "                      probe(h.detach()) -> cls_loss\n",
    "```\n",
    "\n",
    "Записать это можно примерно так:\n",
    "```python\n",
    "for batch in aug_dataloader:\n",
    "  x1, x2, y = batch\n",
    "  h1, h2 = model.backbone(x1, x2)\n",
    "  z1, z2 = model.projection_head(h1, h2)\n",
    "  logits = model.linear_probe(h1.detach())\n",
    "  total_loss = ssl_loss(z1, z2) + cls_loss(yhat, y)\n",
    "  total_loss.backward()\n",
    "```\n",
    "\n",
    "Таким образом, мы сможем в реальном времени наблюдать за тем, как обучение на претекстовую задачу влияет на качество репрезентаций для конечной задачи. Конечно, это не то же самое, что провести полный цикл предобучения, а затем измерить качество на конечной задаче. Тем не менее, это обеспечивает быструю итерацию по конфигурациям гиперпараметров (например, выбор аугментаций). Можно делать запуски на небольшое число эпох и сравнивать онлайн метрики.\n",
    "\n",
    "*NB* Если вспомнить STL-10 из ДЗ 1, разметка была доступна только для небольшого подмножества (`train` vs `unlabeled`). В таком случае онлайн пробинг все еще можно делать, пробу можно обучать во время валидационной эпохи на размеченном сплите (веса энкодера заморожены).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f6f8c4",
   "metadata": {},
   "source": [
    "### Этап отбора аугментаций"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31bec1a-eaff-426c-bb3c-d664064a338d",
   "metadata": {},
   "source": [
    "Добавьте онлайн пробинг в пайплайн обучения SimCLR. Воспользуемся результатами онлайн пробинга на ранних эпохах для отбора аугментаций. Предложите свой набор аугментаций исходя из общих соображений и анализа из **задания 2**, так же можно попробовать [RandAugment](https://docs.pytorch.org/vision/main/generated/torchvision.transforms.RandAugment.html). В качестве референсного набора зафиксируем следующую композицию трансформов:\n",
    "\n",
    "```python\n",
    "Compose(\n",
    "      RandomApply(    \n",
    "          RandomRotation(degrees=[-10.0, 10.0],\n",
    "          interpolation=InterpolationMode.NEAREST,\n",
    "          expand=False,\n",
    "          fill=0))\n",
    "      RandomResizedCrop(\n",
    "          size=(224, 224),\n",
    "          scale=(0.5, 1.0),\n",
    "          ratio=(0.75, 1.3333333333333333),\n",
    "          interpolation=InterpolationMode.BICUBIC, antialias=True)\n",
    "      RandomApply(    \n",
    "          ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2)))\n",
    "      RandomHorizontalFlip(p=0.5)\n",
    "      RandomApply(    \n",
    "          Lambda(<lambda>, types=['object']))\n",
    "      ToTensor()\n",
    "      Normalize(mean=[0.5], std=[0.5], inplace=False)\n",
    "),\n",
    "```\n",
    "где `<lambda>` это функция для инвертирования изображения.\n",
    "Сравните выбранный вами набор аугментаций и референсный, какой из них лучше? Для сравнения можете ориентироваться на метрики онлайн пробинга на 5-10 эпохах."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89749754-1254-4185-a261-c49cc3128c9e",
   "metadata": {},
   "source": [
    "### Этап полного предобучения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136b6272-88fb-4e67-83b6-e205a10120d8",
   "metadata": {},
   "source": [
    "Зафиксируйте \"лучший\" набор и выполните полное предобучение (например, 50 эпох) с методами реализованными **задания 0**: SimCLR и VICReg (не забудьте использовать версию ResNet18 для разрешения 224х224).\n",
    "После предобучения проведите (офлайн) линейный пробинг на всех датасетах (ChestMNIST и PneumoniaMNIST). В отчете продемонстрируйте графики обучения (значение лосса, значение метрик онлайн пробинга в ходе обучения), а также таблицу с финальными результатами. Проанализируйте разницу между SimCLR и VICReg.\n",
    "\n",
    "\n",
    "Итого, краткий план задания:\n",
    "1. Сформируйте собственный набор аугментаций для CXR и добавьте референсный.\n",
    "\n",
    "2. Для каждого набора: предобучение SimCLR 5–10 эпох с онлайн-пробингом; сравниваем метрики.\n",
    "\n",
    "3. Выбираем лучший набор → полное предобучение: SimCLR — 20+ эпох, VICReg — 20+ эпох.\n",
    "\n",
    "4. Выполняем офлайн-линейный пробинг и сравниваем SimCLR и VICReg.\n",
    "\n",
    "**Бонусный балл** получат решения, у которых значения финальных метрик соответсвуют supervised качеству (т.е. как если бы вы обучали ResNet-18 с нуля на каждом датасете). Значения метрик при supervised обучении можно найти [здесь](https://medmnist.com/). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb190c1-a60a-4e5e-a53a-655ad18bff56",
   "metadata": {},
   "source": [
    "#### 1. Сформируйте собственный набор аугментаций для CXR и добавьте референсный."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f736879-cefb-4cf1-a6c6-42179d1ded78",
   "metadata": {},
   "source": [
    "Выполнено в dataset.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeebf12c-0d44-4df2-bb2b-9dd05aa32f19",
   "metadata": {},
   "source": [
    "#### 2. Для каждого набора: предобучение SimCLR 5–10 эпох с онлайн-пробингом; сравниваем метрики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f6ed4e7-749e-4a8d-b0e8-e351f7f4aa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84d83554-cf52-4ebb-8a53-e91da18a2851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from src.dataset import CustomNPZDataset, SSLDataset, get_medmnist_transforms\n",
    "from src.model import SimCLR\n",
    "from src.train import pretrain_ssl_with_online_probe\n",
    "\n",
    "DATA_PATH = './data/pneumoniamnist_224.npz'\n",
    "BATCH_SIZE = 256\n",
    "NUM_WORKERS = 4\n",
    "NUM_CLASSES = 2 # 2 for pneumonia, 14 for ChestMNIST\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "train_base_ds = CustomNPZDataset(DATA_PATH, split='train')\n",
    "val_base_ds = CustomNPZDataset(DATA_PATH, split='val')\n",
    "\n",
    "def run_augmentation_experiment(aug_type, epochs=5):\n",
    "    print(f\"\\n>>> Running Online Probing with Augmentation: {aug_type}\")\n",
    "    ssl_transform = get_medmnist_transforms(size=224, augment=aug_type)\n",
    "    val_transform = get_medmnist_transforms(size=224, augment=None)\n",
    "\n",
    "    train_ssl_ds = SSLDataset(train_base_ds, ssl_transform)\n",
    "\n",
    "\n",
    "    # reinstantiate val to attach transform\n",
    "    val_ds_eval = CustomNPZDataset(DATA_PATH, split='val', transform=val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_ssl_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds_eval, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "    model = SimCLR(encoder_dim=512, projection_dim=128).to(DEVICE)\n",
    "\n",
    "    pretrain_ssl_with_online_probe(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        epochs=epochs,\n",
    "        lr=0.3,\n",
    "        device=DEVICE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc3bbc44-0da1-4f6d-9077-1022fd8337ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Running Online Probing with Augmentation: ref\n",
      "Epoch 1/10 | SSL Loss: 5.8132 | Probe Loss: 0.3997 | Probe Acc: 82.60%\n",
      "Epoch 2/10 | SSL Loss: 5.5437 | Probe Loss: 0.2815 | Probe Acc: 87.96%\n",
      "Epoch 3/10 | SSL Loss: 5.4899 | Probe Loss: 0.2713 | Probe Acc: 88.34%\n",
      "Epoch 4/10 | SSL Loss: 5.4229 | Probe Loss: 0.2604 | Probe Acc: 88.93%\n",
      "Epoch 5/10 | SSL Loss: 5.3532 | Probe Loss: 0.2664 | Probe Acc: 88.85%\n",
      "Val Accuracy: 88.17%\n",
      "Epoch 6/10 | SSL Loss: 5.3300 | Probe Loss: 0.2679 | Probe Acc: 88.91%\n",
      "Epoch 7/10 | SSL Loss: 5.3557 | Probe Loss: 0.2722 | Probe Acc: 88.23%\n",
      "Epoch 8/10 | SSL Loss: 5.2116 | Probe Loss: 0.2627 | Probe Acc: 88.59%\n",
      "Epoch 9/10 | SSL Loss: 5.2414 | Probe Loss: 0.2775 | Probe Acc: 87.83%\n",
      "Epoch 10/10 | SSL Loss: 5.1488 | Probe Loss: 0.2687 | Probe Acc: 88.85%\n",
      "Val Accuracy: 86.07%\n"
     ]
    }
   ],
   "source": [
    "run_augmentation_experiment(aug_type=\"ref\", epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8449233b-aac5-4b53-a590-e39c19b5a752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Running Online Probing with Augmentation: ssl\n",
      "Epoch 1/10 | SSL Loss: 5.6943 | Probe Loss: 0.5744 | Probe Acc: 73.56%\n",
      "Epoch 2/10 | SSL Loss: 5.1378 | Probe Loss: 0.4559 | Probe Acc: 75.79%\n",
      "Epoch 3/10 | SSL Loss: 5.0369 | Probe Loss: 0.3610 | Probe Acc: 84.83%\n",
      "Epoch 4/10 | SSL Loss: 4.9136 | Probe Loss: 0.2968 | Probe Acc: 87.34%\n",
      "Epoch 5/10 | SSL Loss: 4.9104 | Probe Loss: 0.2804 | Probe Acc: 88.23%\n",
      "Val Accuracy: 87.40%\n",
      "Epoch 6/10 | SSL Loss: 4.8125 | Probe Loss: 0.2857 | Probe Acc: 87.70%\n",
      "Epoch 7/10 | SSL Loss: 4.7426 | Probe Loss: 0.2678 | Probe Acc: 89.32%\n",
      "Epoch 8/10 | SSL Loss: 4.7053 | Probe Loss: 0.2576 | Probe Acc: 89.17%\n",
      "Epoch 9/10 | SSL Loss: 4.6375 | Probe Loss: 0.2519 | Probe Acc: 89.25%\n",
      "Epoch 10/10 | SSL Loss: 4.6237 | Probe Loss: 0.2450 | Probe Acc: 89.68%\n",
      "Val Accuracy: 86.45%\n"
     ]
    }
   ],
   "source": [
    "run_augmentation_experiment(aug_type=\"ssl\", epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0ee1fad-99f0-4ca4-acc9-bdbfa09e634f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Running Online Probing with Augmentation: sft\n",
      "Epoch 1/10 | SSL Loss: 5.7531 | Probe Loss: 0.6191 | Probe Acc: 71.45%\n",
      "Epoch 2/10 | SSL Loss: 5.4841 | Probe Loss: 0.4917 | Probe Acc: 74.24%\n",
      "Epoch 3/10 | SSL Loss: 5.2241 | Probe Loss: 0.4099 | Probe Acc: 79.01%\n",
      "Epoch 4/10 | SSL Loss: 4.9952 | Probe Loss: 0.3402 | Probe Acc: 85.94%\n",
      "Epoch 5/10 | SSL Loss: 4.8636 | Probe Loss: 0.3088 | Probe Acc: 87.36%\n",
      "Val Accuracy: 86.83%\n",
      "Epoch 6/10 | SSL Loss: 4.7867 | Probe Loss: 0.2889 | Probe Acc: 88.02%\n",
      "Epoch 7/10 | SSL Loss: 4.7539 | Probe Loss: 0.2885 | Probe Acc: 87.74%\n",
      "Epoch 8/10 | SSL Loss: 4.7504 | Probe Loss: 0.2983 | Probe Acc: 87.21%\n",
      "Epoch 9/10 | SSL Loss: 4.6694 | Probe Loss: 0.2904 | Probe Acc: 87.72%\n",
      "Epoch 10/10 | SSL Loss: 4.6442 | Probe Loss: 0.2921 | Probe Acc: 87.87%\n",
      "Val Accuracy: 85.11%\n"
     ]
    }
   ],
   "source": [
    "run_augmentation_experiment(aug_type=\"sft\", epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28096757-a743-4f12-9e51-802b444fc639",
   "metadata": {},
   "source": [
    "Я устал подбирать уже аугментации для этого задания и я не понимаю, почему мои работают хуже, чем исходные. Будем считать, что мои аугментации получились нормальные. Ран для SFT это аугментации без флипа и менее жоские. Интуитивно мои аугментавции должны работать лучше, как я описал в EDA.ipynb, но почему-то этого не происходит."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a741431-c69b-4121-9042-d5497785a2b1",
   "metadata": {},
   "source": [
    "#### 3. Выбираем лучший набор → полное предобучение: SimCLR — 20+ эпох, VICReg — 20+ эпох.\n",
    "#### 4. Выполняем офлайн-линейный пробинг и сравниваем SimCLR и VICReg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57da9a78-df38-4d21-bd69-6c6c3277d1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from src.model import VICReg\n",
    "from src.train import pretrain_ssl, offline_linear_probe\n",
    "\n",
    "BEST_AUG = \"ssl\"\n",
    "PRETRAIN_EPOCHS = 50\n",
    "EVAL_EPOCHS = 30\n",
    "\n",
    "\n",
    "ssl_transform = get_medmnist_transforms(size=224, augment=BEST_AUG)\n",
    "train_base_ds = CustomNPZDataset(DATA_PATH, split='train')\n",
    "ssl_train_ds = SSLDataset(train_base_ds, ssl_transform)\n",
    "ssl_loader = DataLoader(ssl_train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "clean_transform = get_medmnist_transforms(size=224, augment=None)\n",
    "train_eval_ds = CustomNPZDataset(DATA_PATH, split='train', transform=clean_transform)\n",
    "test_eval_ds = CustomNPZDataset(DATA_PATH, split='test', transform=clean_transform)\n",
    "\n",
    "train_eval_loader = DataLoader(train_eval_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "test_eval_loader = DataLoader(test_eval_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d23de030-9f87-4fa5-b9dd-018c26a88e8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Full Pretraining: SimCLR (50 epochs)\n",
      "Epoch 1/50, Loss: 5.7240\n",
      "Epoch 2/50, Loss: 5.4254\n",
      "Epoch 3/50, Loss: 5.2874\n",
      "Epoch 4/50, Loss: 5.0792\n",
      "Epoch 5/50, Loss: 4.9192\n",
      "Epoch 6/50, Loss: 4.8154\n",
      "Epoch 7/50, Loss: 4.8380\n",
      "Epoch 8/50, Loss: 4.8402\n",
      "Epoch 9/50, Loss: 4.6962\n",
      "Epoch 10/50, Loss: 4.6474\n",
      "Epoch 11/50, Loss: 4.6282\n",
      "Epoch 12/50, Loss: 4.6284\n",
      "Epoch 13/50, Loss: 4.6185\n",
      "Epoch 14/50, Loss: 4.5594\n",
      "Epoch 15/50, Loss: 4.5220\n",
      "Epoch 16/50, Loss: 4.5096\n",
      "Epoch 17/50, Loss: 4.4837\n",
      "Epoch 18/50, Loss: 4.4680\n",
      "Epoch 19/50, Loss: 4.4580\n",
      "Epoch 20/50, Loss: 4.4555\n",
      "Epoch 21/50, Loss: 4.4413\n",
      "Epoch 22/50, Loss: 4.4354\n",
      "Epoch 23/50, Loss: 4.4241\n",
      "Epoch 24/50, Loss: 4.4171\n",
      "Epoch 25/50, Loss: 4.4156\n",
      "Epoch 26/50, Loss: 4.4076\n",
      "Epoch 27/50, Loss: 4.4047\n",
      "Epoch 28/50, Loss: 4.3971\n",
      "Epoch 29/50, Loss: 4.3941\n",
      "Epoch 30/50, Loss: 4.3925\n",
      "Epoch 31/50, Loss: 4.3882\n",
      "Epoch 32/50, Loss: 4.3840\n",
      "Epoch 33/50, Loss: 4.3809\n",
      "Epoch 34/50, Loss: 4.3785\n",
      "Epoch 35/50, Loss: 4.3768\n",
      "Epoch 36/50, Loss: 4.3734\n",
      "Epoch 37/50, Loss: 4.3732\n",
      "Epoch 38/50, Loss: 4.3699\n",
      "Epoch 39/50, Loss: 4.3714\n",
      "Epoch 40/50, Loss: 4.3698\n",
      "Epoch 41/50, Loss: 4.3683\n",
      "Epoch 42/50, Loss: 4.3639\n",
      "Epoch 43/50, Loss: 4.3634\n",
      "Epoch 44/50, Loss: 4.3628\n",
      "Epoch 45/50, Loss: 4.3621\n",
      "Epoch 46/50, Loss: 4.3619\n",
      "Epoch 47/50, Loss: 4.3593\n",
      "Epoch 48/50, Loss: 4.3600\n",
      "Epoch 49/50, Loss: 4.3613\n",
      "Epoch 50/50, Loss: 4.3591\n",
      ">>> Evaluating SimCLR...\n",
      "Epoch 10/30 | Loss: 0.2751 | Acc: 81.73%\n",
      "Epoch 20/30 | Loss: 0.2392 | Acc: 84.46%\n",
      "Epoch 30/30 | Loss: 0.2135 | Acc: 85.42%\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n>>> Full Pretraining: SimCLR ({PRETRAIN_EPOCHS} epochs)\")\n",
    "simclr_model = SimCLR(encoder_dim=512, projection_dim=128).to(DEVICE)\n",
    "simclr_model = pretrain_ssl(simclr_model, ssl_loader, epochs=PRETRAIN_EPOCHS, lr=0.3)\n",
    "\n",
    "print(\">>> Evaluating SimCLR...\")\n",
    "simclr_acc = offline_linear_probe(\n",
    "    simclr_model.encoder,\n",
    "    train_eval_loader,\n",
    "    test_eval_loader,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    epochs=EVAL_EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "607e5570-95d1-4b24-9bed-4dc7f840dfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Full Pretraining: VICReg (50 epochs)\n",
      "Epoch 1/50, Loss: 38.6216\n",
      "Epoch 2/50, Loss: 37.9650\n",
      "Epoch 3/50, Loss: 37.7730\n",
      "Epoch 4/50, Loss: 37.3945\n",
      "Epoch 5/50, Loss: 36.8171\n",
      "Epoch 6/50, Loss: 36.0783\n",
      "Epoch 7/50, Loss: 35.2353\n",
      "Epoch 8/50, Loss: 34.5936\n",
      "Epoch 9/50, Loss: 33.9800\n",
      "Epoch 10/50, Loss: 33.2737\n",
      "Epoch 11/50, Loss: 32.9503\n",
      "Epoch 12/50, Loss: 32.3225\n",
      "Epoch 13/50, Loss: 31.7125\n",
      "Epoch 14/50, Loss: 31.3049\n",
      "Epoch 15/50, Loss: 30.8419\n",
      "Epoch 16/50, Loss: 30.3455\n",
      "Epoch 17/50, Loss: 30.0831\n",
      "Epoch 18/50, Loss: 29.7292\n",
      "Epoch 19/50, Loss: 29.4147\n",
      "Epoch 20/50, Loss: 29.1596\n",
      "Epoch 21/50, Loss: 28.9274\n",
      "Epoch 22/50, Loss: 28.6439\n",
      "Epoch 23/50, Loss: 28.3639\n",
      "Epoch 24/50, Loss: 28.1683\n",
      "Epoch 25/50, Loss: 28.0085\n",
      "Epoch 26/50, Loss: 27.7588\n",
      "Epoch 27/50, Loss: 27.6139\n",
      "Epoch 28/50, Loss: 27.4906\n",
      "Epoch 29/50, Loss: 27.3515\n",
      "Epoch 30/50, Loss: 27.2542\n",
      "Epoch 31/50, Loss: 27.1811\n",
      "Epoch 32/50, Loss: 26.9751\n",
      "Epoch 33/50, Loss: 26.7875\n",
      "Epoch 34/50, Loss: 26.6626\n",
      "Epoch 35/50, Loss: 26.5747\n",
      "Epoch 36/50, Loss: 26.4891\n",
      "Epoch 37/50, Loss: 26.4921\n",
      "Epoch 38/50, Loss: 26.3264\n",
      "Epoch 39/50, Loss: 26.2782\n",
      "Epoch 40/50, Loss: 26.1888\n",
      "Epoch 41/50, Loss: 26.1430\n",
      "Epoch 42/50, Loss: 26.0806\n",
      "Epoch 43/50, Loss: 26.0810\n",
      "Epoch 44/50, Loss: 25.9838\n",
      "Epoch 45/50, Loss: 25.9527\n",
      "Epoch 46/50, Loss: 25.9237\n",
      "Epoch 47/50, Loss: 26.0231\n",
      "Epoch 48/50, Loss: 25.8690\n",
      "Epoch 49/50, Loss: 25.9017\n",
      "Epoch 50/50, Loss: 25.9004\n",
      ">>> Evaluating VICReg...\n",
      "Epoch 10/30 | Loss: 0.2347 | Acc: 87.34%\n",
      "Epoch 20/30 | Loss: 0.1963 | Acc: 88.78%\n",
      "Epoch 30/30 | Loss: 0.1743 | Acc: 88.94%\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n>>> Full Pretraining: VICReg ({PRETRAIN_EPOCHS} epochs)\")\n",
    "\n",
    "vicreg_model = VICReg(encoder_dim=512, projection_dim=2048).to(DEVICE)\n",
    "vicreg_model = pretrain_ssl(vicreg_model, ssl_loader, epochs=PRETRAIN_EPOCHS, lr=0.3)\n",
    "\n",
    "print(\">>> Evaluating VICReg...\")\n",
    "vicreg_acc = offline_linear_probe(\n",
    "    vicreg_model.encoder,\n",
    "    train_eval_loader,\n",
    "    test_eval_loader,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    epochs=EVAL_EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ffc2675-293d-47c0-88e8-7c36402e3868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Results ===\n",
      "SimCLR Top-1 Acc: 85.42%\n",
      "VICReg Top-1 Acc: 88.94%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Final Results ===\")\n",
    "print(f\"SimCLR Top-1 Acc: {simclr_acc:.2f}%\")\n",
    "print(f\"VICReg Top-1 Acc: {vicreg_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a080d9ae-176a-4728-a1e0-e7d4015f6641",
   "metadata": {},
   "source": [
    "## Задание 4 (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0da857-eb1a-4cbb-8c37-727203fb7ee1",
   "metadata": {},
   "source": [
    "Попробуем начать предобучение не с рандомной инициализации, а с весов, полученных предобучением на естественных картинках. Предлагается два варианта на выбор (надо выбрать один):\n",
    "* веса из библиотеки `torchvision`, которые были получены supervised обучением,\n",
    "* веса из соответствующих чекпоинтов [solo-learn](https://github.com/vturrisi/solo-learn/tree/main), которые были получены self-supervised обучением на Imagenet-100 (100-классовая подвыборка ImageNet'а).\n",
    "\n",
    "Для этого при создании энкодера в `torchvision.models.resnet` можно использовать параметр `weights` у `resnet18()`. \n",
    "После инициализации с предобученных весов, проведите такой же цикл предобучения из предыдущего пункта, и продемонстрируйте разницу в финальном качестве. Помогает или вредит старт с supervised imagenet'овских весов?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a06aa803-f3de-4d1e-a41c-ecd503f6bbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 5.6194\n",
      "Epoch 2/50, Loss: 5.0413\n",
      "Epoch 3/50, Loss: 4.9683\n",
      "Epoch 4/50, Loss: 4.8291\n",
      "Epoch 5/50, Loss: 4.7081\n",
      "Epoch 6/50, Loss: 4.6259\n",
      "Epoch 7/50, Loss: 4.6217\n",
      "Epoch 8/50, Loss: 4.5544\n",
      "Epoch 9/50, Loss: 4.5012\n",
      "Epoch 10/50, Loss: 4.4635\n",
      "Epoch 11/50, Loss: 4.4461\n",
      "Epoch 12/50, Loss: 4.4251\n",
      "Epoch 13/50, Loss: 4.4067\n",
      "Epoch 14/50, Loss: 4.3957\n",
      "Epoch 15/50, Loss: 4.3850\n",
      "Epoch 16/50, Loss: 4.3779\n",
      "Epoch 17/50, Loss: 4.3720\n",
      "Epoch 18/50, Loss: 4.3679\n",
      "Epoch 19/50, Loss: 4.3625\n",
      "Epoch 20/50, Loss: 4.3577\n",
      "Epoch 21/50, Loss: 4.3523\n",
      "Epoch 22/50, Loss: 4.3503\n",
      "Epoch 23/50, Loss: 4.3431\n",
      "Epoch 24/50, Loss: 4.3404\n",
      "Epoch 25/50, Loss: 4.3382\n",
      "Epoch 26/50, Loss: 4.3390\n",
      "Epoch 27/50, Loss: 4.3343\n",
      "Epoch 28/50, Loss: 4.3343\n",
      "Epoch 29/50, Loss: 4.3292\n",
      "Epoch 30/50, Loss: 4.3289\n",
      "Epoch 31/50, Loss: 4.3273\n",
      "Epoch 32/50, Loss: 4.3276\n",
      "Epoch 33/50, Loss: 4.3204\n",
      "Epoch 34/50, Loss: 4.3232\n",
      "Epoch 35/50, Loss: 4.3201\n",
      "Epoch 36/50, Loss: 4.3193\n",
      "Epoch 37/50, Loss: 4.3181\n",
      "Epoch 38/50, Loss: 4.3178\n",
      "Epoch 39/50, Loss: 4.3201\n",
      "Epoch 40/50, Loss: 4.3160\n",
      "Epoch 41/50, Loss: 4.3158\n",
      "Epoch 42/50, Loss: 4.3142\n",
      "Epoch 43/50, Loss: 4.3137\n",
      "Epoch 44/50, Loss: 4.3121\n",
      "Epoch 45/50, Loss: 4.3110\n",
      "Epoch 46/50, Loss: 4.3110\n",
      "Epoch 47/50, Loss: 4.3136\n",
      "Epoch 48/50, Loss: 4.3115\n",
      "Epoch 49/50, Loss: 4.3110\n",
      "Epoch 50/50, Loss: 4.3104\n",
      ">>> Evaluating ImageNet-Init SimCLR...\n",
      "Epoch 10/30 | Loss: 0.2491 | Acc: 86.22%\n",
      "Epoch 20/30 | Loss: 0.2274 | Acc: 87.18%\n",
      "Epoch 30/30 | Loss: 0.2136 | Acc: 87.34%\n",
      "\n",
      "=== Initialization Comparison ===\n",
      "Random Init SimCLR:   85.42%\n",
      "ImageNet Init SimCLR: 87.34%\n"
     ]
    }
   ],
   "source": [
    "imagenet_simclr = SimCLR(encoder_dim=512, projection_dim=128, pretrained='imagenet').to(DEVICE)\n",
    "\n",
    "imagenet_simclr = pretrain_ssl(imagenet_simclr, ssl_loader, epochs=PRETRAIN_EPOCHS, lr=0.3)\n",
    "\n",
    "print(\">>> Evaluating ImageNet-Init SimCLR...\")\n",
    "imagenet_acc = offline_linear_probe(\n",
    "    imagenet_simclr.encoder,\n",
    "    train_eval_loader,\n",
    "    test_eval_loader,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    epochs=EVAL_EPOCHS\n",
    ")\n",
    "\n",
    "print(\"\\n=== Initialization Comparison ===\")\n",
    "print(f\"Random Init SimCLR:   {simclr_acc:.2f}%\")\n",
    "print(f\"ImageNet Init SimCLR: {imagenet_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sosal",
   "language": "python",
   "name": "sosal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
