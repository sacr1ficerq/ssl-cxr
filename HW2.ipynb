{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dce818ed",
   "metadata": {},
   "source": [
    "# Домашнее задание 2 \n",
    "## Contrastive and non-contrastive methods in CXR images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc72c49b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Оценивание"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23244fa8-f03c-4e81-92be-fb54a9a20b26",
   "metadata": {},
   "source": [
    "Задание должно быть выполнено самостоятельно. Похожие решения будут считаться плагиатом. Если вы опирались на внешний источник в реализации, необходимо указать ссылку на него. \n",
    "\n",
    "В качестве решения, необходимо предоставить код (`train.py` с аргументами для выбора датасета/метода) + отчет, в котором будут отображены все детали выбора гиперпараметров, комментарии, сопровождающие графики, а так же ответы на вопросы в ДЗ. Оформляйте отчет четко и читаемо. Плохо оформленный код, плохо оформленные графики негативно скажутся на оценке, так же как и неэффективная реализация."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56f10bb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Введение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded48ce1-b7b4-408c-bd9f-f25c12a22b9e",
   "metadata": {},
   "source": [
    "Вам предстоит реализовать (задание 0) и поработать с двумя методами - [SimCLR](https://arxiv.org/abs/2002.05709) и [VICReg](https://arxiv.org/abs/2105.04906). Обучать их будем на датасете, относящемся к домену медицинских изображений (задания 1-4). Подключим онлайн пробинг (задание 3), а так же сравним с трансфером с imagenet домена в этот мед домен (задание 4)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e111a04",
   "metadata": {},
   "source": [
    "### Датасеты [MedMNIST+](https://medmnist.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c31d27-9f38-463d-b8e7-41d815576a7b",
   "metadata": {},
   "source": [
    "Будем использовать уже подготовленные сконвертированные из DICOM'ов картинки. MedMNIST включает в себя два релевантных для нас датасета с рентгеновскими снимками грудной клетки:\n",
    "\n",
    "\n",
    "| MedMNIST2D     | Data Modality | Tasks (# Classes/Labels)           | # Samples | # Training | # Validation | # Test |\n",
    "|----------------|---------------|------------------------------------|-----------|------------|--------------|--------|\n",
    "| ChestMNIST     | Chest X-Ray   | Multi-Label (14), Binary-Class (2) | 112,120   | 78,468     | 11,219       | 22,433 |\n",
    "| PneumoniaMNIST | Chest X-Ray   | Binary-Class (2)                   | 5,856     | 4,708      | 524          | 624    |\n",
    "\n",
    "На этот раз будем использовать разрешение 224x224 (необходимо выставить `size` при инициализации датасета). Несколько картинок из ChestMNIST:\n",
    "\n",
    "![CXR image examples from ChestMNIST](data/cxr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808d9f49",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Задание 0 (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec42ac2-870b-4aa3-b15e-d6de1a2d6ab9",
   "metadata": {},
   "source": [
    "Реализуйте SimCLR и VICReg на базе ResNet-18 энкодера. Для этого надо реализовать соответствующие лосс-функции и архитектуры проекционных голов. Убедитесь, в корректности реализации на CIFAR-10 (не забудьте применить коррекцию резнета для картинок разрешением 32x32 из предыдущего домашнего задания). Для этого, сначала сделайте предобучение на train части датасета в течении 100 эпох, затем сделайте линейный пробинг с замороженным выучившимся энкодером.\n",
    "\n",
    "Референсный интервал top-1 accuracy для 100 эпох предобучения ~80-83% на линейном пробинге (если не получается, проверьте реализацию оптимизатора (**LARS**) и расписания шага обучения (`warmup_cosine`) или попробуйте подвигать learning rate).\n",
    "\n",
    "**NB**\n",
    "Чтобы сэкономить на психотерапевте, используйте оптимизатор [LARS](https://arxiv.org/abs/1708.03888) и `LinearWarmupCosineAnnealing` шедулер. Их нет в торче, но довольно просто реализовать самому или взять референсную реализацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a2e0e25-a7b3-4429-90c3-6343e9b917f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d381201d-9f8a-4d94-b082-0b25c903a69d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 170M/170M [00:46<00:00, 3.67MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 5.3763\n",
      "Epoch 2/100, Loss: 5.1443\n",
      "Epoch 3/100, Loss: 5.1092\n",
      "Epoch 4/100, Loss: 5.0813\n",
      "Epoch 5/100, Loss: 5.0561\n",
      "Epoch 6/100, Loss: 5.0277\n",
      "Epoch 7/100, Loss: 4.9996\n",
      "Epoch 8/100, Loss: 4.9709\n",
      "Epoch 9/100, Loss: 4.9426\n",
      "Epoch 10/100, Loss: 4.9165\n",
      "Epoch 11/100, Loss: 4.8947\n",
      "Epoch 12/100, Loss: 4.8723\n",
      "Epoch 13/100, Loss: 4.8490\n",
      "Epoch 14/100, Loss: 4.8310\n",
      "Epoch 15/100, Loss: 4.8129\n",
      "Epoch 16/100, Loss: 4.7960\n",
      "Epoch 17/100, Loss: 4.7817\n",
      "Epoch 18/100, Loss: 4.7700\n",
      "Epoch 19/100, Loss: 4.7592\n",
      "Epoch 20/100, Loss: 4.7475\n",
      "Epoch 21/100, Loss: 4.7393\n",
      "Epoch 22/100, Loss: 4.7301\n",
      "Epoch 23/100, Loss: 4.7213\n",
      "Epoch 24/100, Loss: 4.7156\n",
      "Epoch 25/100, Loss: 4.7105\n",
      "Epoch 26/100, Loss: 4.7060\n",
      "Epoch 27/100, Loss: 4.6992\n",
      "Epoch 28/100, Loss: 4.6948\n",
      "Epoch 29/100, Loss: 4.6907\n",
      "Epoch 30/100, Loss: 4.6859\n",
      "Epoch 31/100, Loss: 4.6828\n",
      "Epoch 32/100, Loss: 4.6757\n",
      "Epoch 33/100, Loss: 4.6735\n",
      "Epoch 34/100, Loss: 4.6689\n",
      "Epoch 35/100, Loss: 4.6627\n",
      "Epoch 36/100, Loss: 4.6628\n",
      "Epoch 37/100, Loss: 4.6598\n",
      "Epoch 38/100, Loss: 4.6571\n",
      "Epoch 39/100, Loss: 4.6531\n",
      "Epoch 40/100, Loss: 4.6506\n",
      "Epoch 41/100, Loss: 4.6488\n",
      "Epoch 42/100, Loss: 4.6452\n",
      "Epoch 43/100, Loss: 4.6410\n",
      "Epoch 44/100, Loss: 4.6400\n",
      "Epoch 45/100, Loss: 4.6372\n",
      "Epoch 46/100, Loss: 4.6385\n",
      "Epoch 47/100, Loss: 4.6319\n",
      "Epoch 48/100, Loss: 4.6338\n",
      "Epoch 49/100, Loss: 4.6310\n",
      "Epoch 50/100, Loss: 4.6266\n",
      "Epoch 51/100, Loss: 4.6269\n",
      "Epoch 52/100, Loss: 4.6238\n",
      "Epoch 53/100, Loss: 4.6221\n",
      "Epoch 54/100, Loss: 4.6214\n",
      "Epoch 55/100, Loss: 4.6176\n",
      "Epoch 56/100, Loss: 4.6186\n",
      "Epoch 57/100, Loss: 4.6147\n",
      "Epoch 58/100, Loss: 4.6145\n",
      "Epoch 59/100, Loss: 4.6117\n",
      "Epoch 60/100, Loss: 4.6115\n",
      "Epoch 61/100, Loss: 4.6120\n",
      "Epoch 62/100, Loss: 4.6080\n",
      "Epoch 63/100, Loss: 4.6068\n",
      "Epoch 64/100, Loss: 4.6073\n",
      "Epoch 65/100, Loss: 4.6070\n",
      "Epoch 66/100, Loss: 4.6040\n",
      "Epoch 67/100, Loss: 4.6016\n",
      "Epoch 68/100, Loss: 4.6024\n",
      "Epoch 69/100, Loss: 4.6005\n",
      "Epoch 70/100, Loss: 4.6013\n",
      "Epoch 71/100, Loss: 4.5975\n",
      "Epoch 72/100, Loss: 4.5980\n",
      "Epoch 73/100, Loss: 4.5968\n",
      "Epoch 74/100, Loss: 4.5966\n",
      "Epoch 75/100, Loss: 4.5946\n",
      "Epoch 76/100, Loss: 4.5927\n",
      "Epoch 77/100, Loss: 4.5939\n",
      "Epoch 78/100, Loss: 4.5909\n",
      "Epoch 79/100, Loss: 4.5905\n",
      "Epoch 80/100, Loss: 4.5929\n",
      "Epoch 81/100, Loss: 4.5906\n",
      "Epoch 82/100, Loss: 4.5892\n",
      "Epoch 83/100, Loss: 4.5897\n",
      "Epoch 84/100, Loss: 4.5872\n",
      "Epoch 85/100, Loss: 4.5864\n",
      "Epoch 86/100, Loss: 4.5887\n",
      "Epoch 87/100, Loss: 4.5866\n",
      "Epoch 88/100, Loss: 4.5868\n",
      "Epoch 89/100, Loss: 4.5860\n",
      "Epoch 90/100, Loss: 4.5859\n",
      "Epoch 91/100, Loss: 4.5859\n",
      "Epoch 92/100, Loss: 4.5848\n",
      "Epoch 93/100, Loss: 4.5857\n",
      "Epoch 94/100, Loss: 4.5870\n",
      "Epoch 95/100, Loss: 4.5863\n",
      "Epoch 96/100, Loss: 4.5835\n",
      "Epoch 97/100, Loss: 4.5840\n",
      "Epoch 98/100, Loss: 4.5830\n",
      "Epoch 99/100, Loss: 4.5834\n",
      "Epoch 100/100, Loss: 4.5844\n",
      "Epoch 10, Accuracy: 75.95%\n",
      "Epoch 20, Accuracy: 76.84%\n",
      "Epoch 30, Accuracy: 76.77%\n",
      "Epoch 40, Accuracy: 77.49%\n",
      "Epoch 50, Accuracy: 78.02%\n",
      "Epoch 60, Accuracy: 77.89%\n",
      "Epoch 70, Accuracy: 77.82%\n",
      "Epoch 80, Accuracy: 77.72%\n",
      "Epoch 90, Accuracy: 77.78%\n",
      "Epoch 100, Accuracy: 77.88%\n"
     ]
    }
   ],
   "source": [
    "base_dataset = datasets.CIFAR10(root='./data', train=True, download=True)\n",
    "ssl_dataset = SSLDataset(base_dataset, get_ssl_transforms())\n",
    "ssl_loader = DataLoader(ssl_dataset, batch_size=256, shuffle=True, num_workers=4)\n",
    "\n",
    "model = SimCLR()\n",
    "model = pretrain_ssl(model, ssl_loader, epochs=100)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=train_transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=4)\n",
    "\n",
    "linear_probe(model.encoder, train_loader, test_loader, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8803efb6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Задание 1 (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626b15b2-96e6-472b-aaad-0d9177813615",
   "metadata": {},
   "source": [
    "Загрузите упомянутые датасеты из `MedMNIST+` и проанализируйте данные. Например, посмотрите на количество и баланс классов, как устроена разметка по классам, найдите среднее и дисперсию значений пикселей. Определите **подходящие метрики и лосс** для конечной задачи для **каждого** из датасетов, аргументировано объясните ваш выбор."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96c6823-9168-4f90-a959-e320f45db781",
   "metadata": {},
   "source": [
    "Это задание выполнено в \"src/EDA.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaa5120",
   "metadata": {},
   "source": [
    "## Задание 2 (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35463063-b2f6-4e1e-aeb1-4ca1785e1cea",
   "metadata": {},
   "source": [
    "CXR изображения выглядят специфично. Кажется, что нужно иметь и специфичные для таких картинок аугментации.\n",
    "Поиграйтесь с трансформами и зафиксируйте набор, с которым вы будете проводить финальные запуски предобучения.\n",
    "\n",
    "### Каким образом можно определить подходящие аугментации?\n",
    "\n",
    "За неимением экспертного знания (если есть знакомый врач-рентгенолог, можно посоветоваться), будем отталкиваться от набора аугментаций в естественных картинках. Начнем с набора, используемого в SimCLR-подобных методах, для ImageNet. Примерно так готовый набор выглядит в `torchvision`'е (обратите внимание, что при создании `СolorJitter` указываются не сами интвервалы, а дельта, т.е. `brightness=0.4` дает `(0.6, 1.4)`):\n",
    "\n",
    "```python\n",
    "Compose(\n",
    "      RandomResizedCrop(\n",
    "          size=(224, 224),\n",
    "          scale=(0.08, 1.0),\n",
    "          ratio=(0.75, 1.3333333333333333),\n",
    "          interpolation=InterpolationMode.BICUBIC,\n",
    "          antialias=True)\n",
    "      RandomApply(\n",
    "          ColorJitter(\n",
    "              brightness=(0.6, 1.4),\n",
    "              contrast=(0.6, 1.4),\n",
    "              saturation=(0.8, 1.2),\n",
    "              hue=(-0.1, 0.1)))\n",
    "      RandomGrayscale(p=0.2)\n",
    "      GaussianBlur(p=0.5)\n",
    "      Solarization(p=0.1)\n",
    "      RandomHorizontalFlip(p=0.5)\n",
    "      ToTensor()\n",
    "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.228, 0.224, 0.225], inplace=False)\n",
    "```\n",
    "\n",
    "Какие параметры аугментаций стоило бы поменять? Реализуйте набор трансформов, посмотрите какие картинки получаются на выходе (не забудьте перевести выход в нужный интервал значений для визуализации), поиграйтесь со значениями параметров (например, `scale` в `RandomResizedCrop` или `brightness` в `ColorJitter`). Какие трансформы стоит убрать? Попробуйте добавить инвертирование и повороты картинок на небольшой угол."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f218776-5638-4726-9905-f8edf9c47367",
   "metadata": {},
   "source": [
    "Рентгеновский снимок - это не отражение света, а проекция плотности тканей, поэтому дефолтный набор аугментаций будет работать плохо. \n",
    "\n",
    "Остальной анализ и подбор аугментаций описан в src/EDA.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7ed726",
   "metadata": {},
   "source": [
    "## Задание 3 (4 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538d28e6-c567-45a3-b5b6-293630dd1cfb",
   "metadata": {},
   "source": [
    "Для того, чтобы честно найти подходящий набор аугментаций, надо проводить этап предобучения и затем оценивать качество получившихся репрезентаций на конечной задаче. Можно ли это как-то ускорить? Раз у нас есть разметка для всего датасета, воспользуемся ей и ускорим подбор аугментаций с помощью online probing'a.\n",
    "Для этого, добавим голову для линейного пробинга `linear_probe` к нашему энкодеру (`backbone`) и проекционной голове (`projection_head`). Эта линейная \"проба\" будет состоять из одного линейного слоя из размерности выхода энкодера (например, 512 для ResNet18) в число классов на конечной задаче (например, 14 классов у ChestMNIST)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881f35e8",
   "metadata": {},
   "source": [
    "На каждой итерации **предобучения** будем учить `backbone` и `projection_head` на претекстовую задачу (например, SimCLR лосс), а линейную пробу на классификацию.\n",
    "Получается такая двуглавая архитектура, где градиенты с претекстового лосса текут по проекционной голове и энкодеру, а градиенты с классификационного лосса только по линейной пробе (не забудьте правильно `detach'`нуться).\n",
    "\n",
    "```\n",
    "                      projection_head(h) -> ssl_loss\n",
    "                    /\n",
    "x -> encoder(x) -> h\n",
    "                    \\\n",
    "                      probe(h.detach()) -> cls_loss\n",
    "```\n",
    "\n",
    "Записать это можно примерно так:\n",
    "```python\n",
    "for batch in aug_dataloader:\n",
    "  x1, x2, y = batch\n",
    "  h1, h2 = model.backbone(x1, x2)\n",
    "  z1, z2 = model.projection_head(h1, h2)\n",
    "  logits = model.linear_probe(h1.detach())\n",
    "  total_loss = ssl_loss(z1, z2) + cls_loss(yhat, y)\n",
    "  total_loss.backward()\n",
    "```\n",
    "\n",
    "Таким образом, мы сможем в реальном времени наблюдать за тем, как обучение на претекстовую задачу влияет на качество репрезентаций для конечной задачи. Конечно, это не то же самое, что провести полный цикл предобучения, а затем измерить качество на конечной задаче. Тем не менее, это обеспечивает быструю итерацию по конфигурациям гиперпараметров (например, выбор аугментаций). Можно делать запуски на небольшое число эпох и сравнивать онлайн метрики.\n",
    "\n",
    "*NB* Если вспомнить STL-10 из ДЗ 1, разметка была доступна только для небольшого подмножества (`train` vs `unlabeled`). В таком случае онлайн пробинг все еще можно делать, пробу можно обучать во время валидационной эпохи на размеченном сплите (веса энкодера заморожены).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f6f8c4",
   "metadata": {},
   "source": [
    "### Этап отбора аугментаций"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31bec1a-eaff-426c-bb3c-d664064a338d",
   "metadata": {},
   "source": [
    "Добавьте онлайн пробинг в пайплайн обучения SimCLR. Воспользуемся результатами онлайн пробинга на ранних эпохах для отбора аугментаций. Предложите свой набор аугментаций исходя из общих соображений и анализа из **задания 2**, так же можно попробовать [RandAugment](https://docs.pytorch.org/vision/main/generated/torchvision.transforms.RandAugment.html). В качестве референсного набора зафиксируем следующую композицию трансформов:\n",
    "\n",
    "```python\n",
    "Compose(\n",
    "      RandomApply(    \n",
    "          RandomRotation(degrees=[-10.0, 10.0],\n",
    "          interpolation=InterpolationMode.NEAREST,\n",
    "          expand=False,\n",
    "          fill=0))\n",
    "      RandomResizedCrop(\n",
    "          size=(224, 224),\n",
    "          scale=(0.5, 1.0),\n",
    "          ratio=(0.75, 1.3333333333333333),\n",
    "          interpolation=InterpolationMode.BICUBIC, antialias=True)\n",
    "      RandomApply(    \n",
    "          ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2)))\n",
    "      RandomHorizontalFlip(p=0.5)\n",
    "      RandomApply(    \n",
    "          Lambda(<lambda>, types=['object']))\n",
    "      ToTensor()\n",
    "      Normalize(mean=[0.5], std=[0.5], inplace=False)\n",
    "),\n",
    "```\n",
    "где `<lambda>` это функция для инвертирования изображения.\n",
    "Сравните выбранный вами набор аугментаций и референсный, какой из них лучше? Для сравнения можете ориентироваться на метрики онлайн пробинга на 5-10 эпохах."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89749754-1254-4185-a261-c49cc3128c9e",
   "metadata": {},
   "source": [
    "### Этап полного предобучения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136b6272-88fb-4e67-83b6-e205a10120d8",
   "metadata": {},
   "source": [
    "Зафиксируйте \"лучший\" набор и выполните полное предобучение (например, 50 эпох) с методами реализованными **задания 0**: SimCLR и VICReg (не забудьте использовать версию ResNet18 для разрешения 224х224).\n",
    "После предобучения проведите (офлайн) линейный пробинг на всех датасетах (ChestMNIST и PneumoniaMNIST). В отчете продемонстрируйте графики обучения (значение лосса, значение метрик онлайн пробинга в ходе обучения), а также таблицу с финальными результатами. Проанализируйте разницу между SimCLR и VICReg.\n",
    "\n",
    "\n",
    "Итого, краткий план задания:\n",
    "1. Сформируйте собственный набор аугментаций для CXR и добавьте референсный.\n",
    "\n",
    "2. Для каждого набора: предобучение SimCLR 5–10 эпох с онлайн-пробингом; сравниваем метрики.\n",
    "\n",
    "3. Выбираем лучший набор → полное предобучение: SimCLR — 20+ эпох, VICReg — 20+ эпох.\n",
    "\n",
    "4. Выполняем офлайн-линейный пробинг и сравниваем SimCLR и VICReg.\n",
    "\n",
    "**Бонусный балл** получат решения, у которых значения финальных метрик соответсвуют supervised качеству (т.е. как если бы вы обучали ResNet-18 с нуля на каждом датасете). Значения метрик при supervised обучении можно найти [здесь](https://medmnist.com/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1936a7c1-b45d-471b-b2db-e9785ac4bff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import CustomNPZDataset, SSLDataset, HFDataset, get_medmnist_transforms\n",
    "from src.model import SimCLRMedMNIST, VICRegMedMNIST\n",
    "from src.train import pretrain_ssl_with_online_probe, offline_linear_probe\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "# chest_path = 'data/chestmnist_224.npz'\n",
    "pneumonia_path = 'data/pneumoniamnist_224.npz'\n",
    "chest_path = pneumonia_path\n",
    "\n",
    "train_base = CustomNPZDataset(chest_path, split='train', transform=None)\n",
    "val_base = CustomNPZDataset(chest_path, split='val', transform=None)\n",
    "test_base = CustomNPZDataset(chest_path, split='test', transform=None)\n",
    "\n",
    "ssl_transform = get_medmnist_transforms(size=224, augment=True)\n",
    "train_ssl = HFDataset(SSLDataset(train_base, ssl_transform), for_ssl=True)\n",
    "val_ssl = HFDataset(SSLDataset(val_base, ssl_transform), for_ssl=True)\n",
    "\n",
    "train_loader = DataLoader(train_ssl, batch_size=256, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_ssl, batch_size=256, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a9e28d4-67e3-4ae4-a2e8-cc286b28a3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Online probing\n",
    "model = SimCLRMedMNIST(encoder_dim=512, pretrained=None).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "222a4dfe-3d6b-470b-ae27-76af87a535d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 196.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 103.38 MiB is free. Including non-PyTorch memory, this process has 8.76 GiB memory in use. Of the allocated memory 8.17 GiB is allocated by PyTorch, and 437.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mpretrain_ssl_with_online_probe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m14\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/HSE/SSL/hw2/src/train.py:203\u001b[0m, in \u001b[0;36mpretrain_ssl_with_online_probe\u001b[0;34m(model, train_loader, val_loader, num_classes, epochs, lr, device)\u001b[0m\n\u001b[1;32m    200\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    201\u001b[0m probe_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 203\u001b[0m ssl_loss \u001b[38;5;241m=\u001b[39m \u001b[43mssl_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mview1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mview2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m ssl_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    205\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Desktop/HSE/SSL/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/HSE/SSL/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/HSE/SSL/hw2/src/model.py:153\u001b[0m, in \u001b[0;36mSimCLRMedMNIST.forward\u001b[0;34m(self, view1, view2)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, view1, view2):\n\u001b[1;32m    152\u001b[0m     z1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojection_head(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(view1))\n\u001b[0;32m--> 153\u001b[0m     z2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojection_head(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mview2\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    154\u001b[0m     z1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(z1, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    155\u001b[0m     z2 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(z2, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/HSE/SSL/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/HSE/SSL/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/HSE/SSL/venv/lib/python3.10/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/HSE/SSL/venv/lib/python3.10/site-packages/torchvision/models/resnet.py:273\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    270\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m    271\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n\u001b[0;32m--> 273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[1;32m    275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n",
      "File \u001b[0;32m~/Desktop/HSE/SSL/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/HSE/SSL/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/HSE/SSL/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:244\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 244\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/HSE/SSL/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/HSE/SSL/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/HSE/SSL/venv/lib/python3.10/site-packages/torchvision/models/resnet.py:92\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     90\u001b[0m     identity \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m---> 92\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[1;32m     94\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/Desktop/HSE/SSL/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/HSE/SSL/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/HSE/SSL/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:548\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 548\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/HSE/SSL/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:543\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    533\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    534\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    542\u001b[0m     )\n\u001b[0;32m--> 543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 196.00 MiB. GPU 0 has a total capacity of 10.90 GiB of which 103.38 MiB is free. Including non-PyTorch memory, this process has 8.76 GiB memory in use. Of the allocated memory 8.17 GiB is allocated by PyTorch, and 437.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "model = pretrain_ssl_with_online_probe(model, \n",
    "                                       train_loader,\n",
    "                                       val_loader, \n",
    "                                       num_classes=14,\n",
    "                                       epochs=50,\n",
    "                                       device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a77f09-fa7c-4772-b409-361d16694a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: With ImageNet initialization\n",
    "model_imagenet = SimCLRMedMNIST(encoder_dim=512, pretrained='imagenet')\n",
    "model_imagenet = pretrain_ssl_with_online_probe(model_imagenet, train_loader, val_loader,\n",
    "                                                 num_classes=14, epochs=50, device=device)\n",
    "\n",
    "# Offline evaluation\n",
    "eval_transform = get_medmnist_transforms(size=224, augment=False)\n",
    "train_eval = HFDataset(CustomNPZDataset(chest_path, 'train', eval_transform), for_ssl=False)\n",
    "test_eval = HFDataset(CustomNPZDataset(chest_path, 'test', eval_transform), for_ssl=False)\n",
    "train_eval_loader = DataLoader(train_eval, batch_size=256, shuffle=True, num_workers=4)\n",
    "test_eval_loader = DataLoader(test_eval, batch_size=256, shuffle=False, num_workers=4)\n",
    "\n",
    "acc = offline_linear_probe(model.encoder, train_eval_loader, test_eval_loader, \n",
    "                           num_classes=14, epochs=100, device=device)\n",
    "print(f\"Final accuracy: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85b5fa8",
   "metadata": {},
   "source": [
    "## Задание 4 (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc2d4d7-8af8-4ef7-ae05-6b6592dd1730",
   "metadata": {},
   "source": [
    "Попробуем начать предобучение не с рандомной инициализации, а с весов, полученных предобучением на естественных картинках. Предлагается два варианта на выбор (надо выбрать один):\n",
    "* веса из библиотеки `torchvision`, которые были получены supervised обучением,\n",
    "* веса из соответствующих чекпоинтов [solo-learn](https://github.com/vturrisi/solo-learn/tree/main), которые были получены self-supervised обучением на Imagenet-100 (100-классовая подвыборка ImageNet'а).\n",
    "\n",
    "Для этого при создании энкодера в `torchvision.models.resnet` можно использовать параметр `weights` у `resnet18()`. \n",
    "После инициализации с предобученных весов, проведите такой же цикл предобучения из предыдущего пункта, и продемонстрируйте разницу в финальном качестве. Помогает или вредит старт с supervised imagenet'овских весов?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06aa803-f3de-4d1e-a41c-ecd503f6bbc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SSL",
   "language": "python",
   "name": "ssl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
