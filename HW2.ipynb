{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dce818ed",
   "metadata": {},
   "source": [
    "# Домашнее задание 2 \n",
    "## Contrastive and non-contrastive methods in CXR images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc72c49b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Оценивание"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23244fa8-f03c-4e81-92be-fb54a9a20b26",
   "metadata": {},
   "source": [
    "Задание должно быть выполнено самостоятельно. Похожие решения будут считаться плагиатом. Если вы опирались на внешний источник в реализации, необходимо указать ссылку на него. \n",
    "\n",
    "В качестве решения, необходимо предоставить код (`train.py` с аргументами для выбора датасета/метода) + отчет, в котором будут отображены все детали выбора гиперпараметров, комментарии, сопровождающие графики, а так же ответы на вопросы в ДЗ. Оформляйте отчет четко и читаемо. Плохо оформленный код, плохо оформленные графики негативно скажутся на оценке, так же как и неэффективная реализация."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56f10bb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Введение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded48ce1-b7b4-408c-bd9f-f25c12a22b9e",
   "metadata": {},
   "source": [
    "Вам предстоит реализовать (задание 0) и поработать с двумя методами - [SimCLR](https://arxiv.org/abs/2002.05709) и [VICReg](https://arxiv.org/abs/2105.04906). Обучать их будем на датасете, относящемся к домену медицинских изображений (задания 1-4). Подключим онлайн пробинг (задание 3), а так же сравним с трансфером с imagenet домена в этот мед домен (задание 4)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e111a04",
   "metadata": {},
   "source": [
    "### Датасеты [MedMNIST+](https://medmnist.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c31d27-9f38-463d-b8e7-41d815576a7b",
   "metadata": {},
   "source": [
    "Будем использовать уже подготовленные сконвертированные из DICOM'ов картинки. MedMNIST включает в себя два релевантных для нас датасета с рентгеновскими снимками грудной клетки:\n",
    "\n",
    "\n",
    "| MedMNIST2D     | Data Modality | Tasks (# Classes/Labels)           | # Samples | # Training | # Validation | # Test |\n",
    "|----------------|---------------|------------------------------------|-----------|------------|--------------|--------|\n",
    "| ChestMNIST     | Chest X-Ray   | Multi-Label (14), Binary-Class (2) | 112,120   | 78,468     | 11,219       | 22,433 |\n",
    "| PneumoniaMNIST | Chest X-Ray   | Binary-Class (2)                   | 5,856     | 4,708      | 524          | 624    |\n",
    "\n",
    "На этот раз будем использовать разрешение 224x224 (необходимо выставить `size` при инициализации датасета). Несколько картинок из ChestMNIST:\n",
    "\n",
    "![CXR image examples from ChestMNIST](data/cxr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808d9f49",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Задание 0 (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec42ac2-870b-4aa3-b15e-d6de1a2d6ab9",
   "metadata": {},
   "source": [
    "Реализуйте SimCLR и VICReg на базе ResNet-18 энкодера. Для этого надо реализовать соответствующие лосс-функции и архитектуры проекционных голов. Убедитесь, в корректности реализации на CIFAR-10 (не забудьте применить коррекцию резнета для картинок разрешением 32x32 из предыдущего домашнего задания). Для этого, сначала сделайте предобучение на train части датасета в течении 100 эпох, затем сделайте линейный пробинг с замороженным выучившимся энкодером.\n",
    "\n",
    "Референсный интервал top-1 accuracy для 100 эпох предобучения ~80-83% на линейном пробинге (если не получается, проверьте реализацию оптимизатора (**LARS**) и расписания шага обучения (`warmup_cosine`) или попробуйте подвигать learning rate).\n",
    "\n",
    "**NB**\n",
    "Чтобы сэкономить на психотерапевте, используйте оптимизатор [LARS](https://arxiv.org/abs/1708.03888) и `LinearWarmupCosineAnnealing` шедулер. Их нет в торче, но довольно просто реализовать самому или взять референсную реализацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a2e0e25-a7b3-4429-90c3-6343e9b917f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d381201d-9f8a-4d94-b082-0b25c903a69d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 170M/170M [00:46<00:00, 3.67MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 5.3763\n",
      "Epoch 2/100, Loss: 5.1443\n",
      "Epoch 3/100, Loss: 5.1092\n",
      "Epoch 4/100, Loss: 5.0813\n",
      "Epoch 5/100, Loss: 5.0561\n",
      "Epoch 6/100, Loss: 5.0277\n",
      "Epoch 7/100, Loss: 4.9996\n",
      "Epoch 8/100, Loss: 4.9709\n",
      "Epoch 9/100, Loss: 4.9426\n",
      "Epoch 10/100, Loss: 4.9165\n",
      "Epoch 11/100, Loss: 4.8947\n",
      "Epoch 12/100, Loss: 4.8723\n",
      "Epoch 13/100, Loss: 4.8490\n",
      "Epoch 14/100, Loss: 4.8310\n",
      "Epoch 15/100, Loss: 4.8129\n",
      "Epoch 16/100, Loss: 4.7960\n",
      "Epoch 17/100, Loss: 4.7817\n",
      "Epoch 18/100, Loss: 4.7700\n",
      "Epoch 19/100, Loss: 4.7592\n",
      "Epoch 20/100, Loss: 4.7475\n",
      "Epoch 21/100, Loss: 4.7393\n",
      "Epoch 22/100, Loss: 4.7301\n",
      "Epoch 23/100, Loss: 4.7213\n",
      "Epoch 24/100, Loss: 4.7156\n",
      "Epoch 25/100, Loss: 4.7105\n",
      "Epoch 26/100, Loss: 4.7060\n",
      "Epoch 27/100, Loss: 4.6992\n",
      "Epoch 28/100, Loss: 4.6948\n",
      "Epoch 29/100, Loss: 4.6907\n",
      "Epoch 30/100, Loss: 4.6859\n",
      "Epoch 31/100, Loss: 4.6828\n",
      "Epoch 32/100, Loss: 4.6757\n",
      "Epoch 33/100, Loss: 4.6735\n",
      "Epoch 34/100, Loss: 4.6689\n",
      "Epoch 35/100, Loss: 4.6627\n",
      "Epoch 36/100, Loss: 4.6628\n",
      "Epoch 37/100, Loss: 4.6598\n",
      "Epoch 38/100, Loss: 4.6571\n",
      "Epoch 39/100, Loss: 4.6531\n",
      "Epoch 40/100, Loss: 4.6506\n",
      "Epoch 41/100, Loss: 4.6488\n",
      "Epoch 42/100, Loss: 4.6452\n",
      "Epoch 43/100, Loss: 4.6410\n",
      "Epoch 44/100, Loss: 4.6400\n",
      "Epoch 45/100, Loss: 4.6372\n",
      "Epoch 46/100, Loss: 4.6385\n",
      "Epoch 47/100, Loss: 4.6319\n",
      "Epoch 48/100, Loss: 4.6338\n",
      "Epoch 49/100, Loss: 4.6310\n",
      "Epoch 50/100, Loss: 4.6266\n",
      "Epoch 51/100, Loss: 4.6269\n",
      "Epoch 52/100, Loss: 4.6238\n",
      "Epoch 53/100, Loss: 4.6221\n",
      "Epoch 54/100, Loss: 4.6214\n",
      "Epoch 55/100, Loss: 4.6176\n",
      "Epoch 56/100, Loss: 4.6186\n",
      "Epoch 57/100, Loss: 4.6147\n",
      "Epoch 58/100, Loss: 4.6145\n",
      "Epoch 59/100, Loss: 4.6117\n",
      "Epoch 60/100, Loss: 4.6115\n",
      "Epoch 61/100, Loss: 4.6120\n",
      "Epoch 62/100, Loss: 4.6080\n",
      "Epoch 63/100, Loss: 4.6068\n",
      "Epoch 64/100, Loss: 4.6073\n",
      "Epoch 65/100, Loss: 4.6070\n",
      "Epoch 66/100, Loss: 4.6040\n",
      "Epoch 67/100, Loss: 4.6016\n",
      "Epoch 68/100, Loss: 4.6024\n",
      "Epoch 69/100, Loss: 4.6005\n",
      "Epoch 70/100, Loss: 4.6013\n",
      "Epoch 71/100, Loss: 4.5975\n",
      "Epoch 72/100, Loss: 4.5980\n",
      "Epoch 73/100, Loss: 4.5968\n",
      "Epoch 74/100, Loss: 4.5966\n",
      "Epoch 75/100, Loss: 4.5946\n",
      "Epoch 76/100, Loss: 4.5927\n",
      "Epoch 77/100, Loss: 4.5939\n",
      "Epoch 78/100, Loss: 4.5909\n",
      "Epoch 79/100, Loss: 4.5905\n",
      "Epoch 80/100, Loss: 4.5929\n",
      "Epoch 81/100, Loss: 4.5906\n",
      "Epoch 82/100, Loss: 4.5892\n",
      "Epoch 83/100, Loss: 4.5897\n",
      "Epoch 84/100, Loss: 4.5872\n",
      "Epoch 85/100, Loss: 4.5864\n",
      "Epoch 86/100, Loss: 4.5887\n",
      "Epoch 87/100, Loss: 4.5866\n",
      "Epoch 88/100, Loss: 4.5868\n",
      "Epoch 89/100, Loss: 4.5860\n",
      "Epoch 90/100, Loss: 4.5859\n",
      "Epoch 91/100, Loss: 4.5859\n",
      "Epoch 92/100, Loss: 4.5848\n",
      "Epoch 93/100, Loss: 4.5857\n",
      "Epoch 94/100, Loss: 4.5870\n",
      "Epoch 95/100, Loss: 4.5863\n",
      "Epoch 96/100, Loss: 4.5835\n",
      "Epoch 97/100, Loss: 4.5840\n",
      "Epoch 98/100, Loss: 4.5830\n",
      "Epoch 99/100, Loss: 4.5834\n",
      "Epoch 100/100, Loss: 4.5844\n",
      "Epoch 10, Accuracy: 75.95%\n",
      "Epoch 20, Accuracy: 76.84%\n",
      "Epoch 30, Accuracy: 76.77%\n",
      "Epoch 40, Accuracy: 77.49%\n",
      "Epoch 50, Accuracy: 78.02%\n",
      "Epoch 60, Accuracy: 77.89%\n",
      "Epoch 70, Accuracy: 77.82%\n",
      "Epoch 80, Accuracy: 77.72%\n",
      "Epoch 90, Accuracy: 77.78%\n",
      "Epoch 100, Accuracy: 77.88%\n"
     ]
    }
   ],
   "source": [
    "base_dataset = datasets.CIFAR10(root='./data', train=True, download=True)\n",
    "ssl_dataset = SSLDataset(base_dataset, get_ssl_transforms())\n",
    "ssl_loader = DataLoader(ssl_dataset, batch_size=256, shuffle=True, num_workers=4)\n",
    "\n",
    "model = SimCLR()\n",
    "model = pretrain_ssl(model, ssl_loader, epochs=100)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=train_transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=4)\n",
    "\n",
    "linear_probe(model.encoder, train_loader, test_loader, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8803efb6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Задание 1 (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626b15b2-96e6-472b-aaad-0d9177813615",
   "metadata": {},
   "source": [
    "Загрузите упомянутые датасеты из `MedMNIST+` и проанализируйте данные. Например, посмотрите на количество и баланс классов, как устроена разметка по классам, найдите среднее и дисперсию значений пикселей. Определите **подходящие метрики и лосс** для конечной задачи для **каждого** из датасетов, аргументировано объясните ваш выбор."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96c6823-9168-4f90-a959-e320f45db781",
   "metadata": {},
   "source": [
    "Это задание выполнено в \"src/EDA.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaa5120",
   "metadata": {},
   "source": [
    "## Задание 2 (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35463063-b2f6-4e1e-aeb1-4ca1785e1cea",
   "metadata": {},
   "source": [
    "CXR изображения выглядят специфично. Кажется, что нужно иметь и специфичные для таких картинок аугментации.\n",
    "Поиграйтесь с трансформами и зафиксируйте набор, с которым вы будете проводить финальные запуски предобучения.\n",
    "\n",
    "### Каким образом можно определить подходящие аугментации?\n",
    "\n",
    "За неимением экспертного знания (если есть знакомый врач-рентгенолог, можно посоветоваться), будем отталкиваться от набора аугментаций в естественных картинках. Начнем с набора, используемого в SimCLR-подобных методах, для ImageNet. Примерно так готовый набор выглядит в `torchvision`'е (обратите внимание, что при создании `СolorJitter` указываются не сами интвервалы, а дельта, т.е. `brightness=0.4` дает `(0.6, 1.4)`):\n",
    "\n",
    "```python\n",
    "Compose(\n",
    "      RandomResizedCrop(\n",
    "          size=(224, 224),\n",
    "          scale=(0.08, 1.0),\n",
    "          ratio=(0.75, 1.3333333333333333),\n",
    "          interpolation=InterpolationMode.BICUBIC,\n",
    "          antialias=True)\n",
    "      RandomApply(\n",
    "          ColorJitter(\n",
    "              brightness=(0.6, 1.4),\n",
    "              contrast=(0.6, 1.4),\n",
    "              saturation=(0.8, 1.2),\n",
    "              hue=(-0.1, 0.1)))\n",
    "      RandomGrayscale(p=0.2)\n",
    "      GaussianBlur(p=0.5)\n",
    "      Solarization(p=0.1)\n",
    "      RandomHorizontalFlip(p=0.5)\n",
    "      ToTensor()\n",
    "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.228, 0.224, 0.225], inplace=False)\n",
    "```\n",
    "\n",
    "Какие параметры аугментаций стоило бы поменять? Реализуйте набор трансформов, посмотрите какие картинки получаются на выходе (не забудьте перевести выход в нужный интервал значений для визуализации), поиграйтесь со значениями параметров (например, `scale` в `RandomResizedCrop` или `brightness` в `ColorJitter`). Какие трансформы стоит убрать? Попробуйте добавить инвертирование и повороты картинок на небольшой угол."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f218776-5638-4726-9905-f8edf9c47367",
   "metadata": {},
   "source": [
    "Рентгеновский снимок - это не отражение света, а проекция плотности тканей, поэтому дефолтный набор аугментаций будет работать плохо. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56189945-befd-4aca-89e0-ce5edabd8b76",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0b3b5a-b807-418c-b32f-c3172b0b2996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d7ed726",
   "metadata": {},
   "source": [
    "## Задание 3 (4 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538d28e6-c567-45a3-b5b6-293630dd1cfb",
   "metadata": {},
   "source": [
    "Для того, чтобы честно найти подходящий набор аугментаций, надо проводить этап предобучения и затем оценивать качество получившихся репрезентаций на конечной задаче. Можно ли это как-то ускорить? Раз у нас есть разметка для всего датасета, воспользуемся ей и ускорим подбор аугментаций с помощью online probing'a.\n",
    "Для этого, добавим голову для линейного пробинга `linear_probe` к нашему энкодеру (`backbone`) и проекционной голове (`projection_head`). Эта линейная \"проба\" будет состоять из одного линейного слоя из размерности выхода энкодера (например, 512 для ResNet18) в число классов на конечной задаче (например, 14 классов у ChestMNIST)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881f35e8",
   "metadata": {},
   "source": [
    "На каждой итерации **предобучения** будем учить `backbone` и `projection_head` на претекстовую задачу (например, SimCLR лосс), а линейную пробу на классификацию.\n",
    "Получается такая двуглавая архитектура, где градиенты с претекстового лосса текут по проекционной голове и энкодеру, а градиенты с классификационного лосса только по линейной пробе (не забудьте правильно `detach'`нуться).\n",
    "\n",
    "```\n",
    "                      projection_head(h) -> ssl_loss\n",
    "                    /\n",
    "x -> encoder(x) -> h\n",
    "                    \\\n",
    "                      probe(h.detach()) -> cls_loss\n",
    "```\n",
    "\n",
    "Записать это можно примерно так:\n",
    "```python\n",
    "for batch in aug_dataloader:\n",
    "  x1, x2, y = batch\n",
    "  h1, h2 = model.backbone(x1, x2)\n",
    "  z1, z2 = model.projection_head(h1, h2)\n",
    "  logits = model.linear_probe(h1.detach())\n",
    "  total_loss = ssl_loss(z1, z2) + cls_loss(yhat, y)\n",
    "  total_loss.backward()\n",
    "```\n",
    "\n",
    "Таким образом, мы сможем в реальном времени наблюдать за тем, как обучение на претекстовую задачу влияет на качество репрезентаций для конечной задачи. Конечно, это не то же самое, что провести полный цикл предобучения, а затем измерить качество на конечной задаче. Тем не менее, это обеспечивает быструю итерацию по конфигурациям гиперпараметров (например, выбор аугментаций). Можно делать запуски на небольшое число эпох и сравнивать онлайн метрики.\n",
    "\n",
    "*NB* Если вспомнить STL-10 из ДЗ 1, разметка была доступна только для небольшого подмножества (`train` vs `unlabeled`). В таком случае онлайн пробинг все еще можно делать, пробу можно обучать во время валидационной эпохи на размеченном сплите (веса энкодера заморожены).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f6f8c4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Этап отбора аугментаций"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31bec1a-eaff-426c-bb3c-d664064a338d",
   "metadata": {},
   "source": [
    "Добавьте онлайн пробинг в пайплайн обучения SimCLR. Воспользуемся результатами онлайн пробинга на ранних эпохах для отбора аугментаций. Предложите свой набор аугментаций исходя из общих соображений и анализа из **задания 2**, так же можно попробовать [RandAugment](https://docs.pytorch.org/vision/main/generated/torchvision.transforms.RandAugment.html). В качестве референсного набора зафиксируем следующую композицию трансформов:\n",
    "\n",
    "```python\n",
    "Compose(\n",
    "      RandomApply(    \n",
    "          RandomRotation(degrees=[-10.0, 10.0],\n",
    "          interpolation=InterpolationMode.NEAREST,\n",
    "          expand=False,\n",
    "          fill=0))\n",
    "      RandomResizedCrop(\n",
    "          size=(224, 224),\n",
    "          scale=(0.5, 1.0),\n",
    "          ratio=(0.75, 1.3333333333333333),\n",
    "          interpolation=InterpolationMode.BICUBIC, antialias=True)\n",
    "      RandomApply(    \n",
    "          ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2)))\n",
    "      RandomHorizontalFlip(p=0.5)\n",
    "      RandomApply(    \n",
    "          Lambda(<lambda>, types=['object']))\n",
    "      ToTensor()\n",
    "      Normalize(mean=[0.5], std=[0.5], inplace=False)\n",
    "),\n",
    "```\n",
    "где `<lambda>` это функция для инвертирования изображения.\n",
    "Сравните выбранный вами набор аугментаций и референсный, какой из них лучше? Для сравнения можете ориентироваться на метрики онлайн пробинга на 5-10 эпохах."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89749754-1254-4185-a261-c49cc3128c9e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Этап полного предобучения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136b6272-88fb-4e67-83b6-e205a10120d8",
   "metadata": {},
   "source": [
    "Зафиксируйте \"лучший\" набор и выполните полное предобучение (например, 50 эпох) с методами реализованными **задания 0**: SimCLR и VICReg (не забудьте использовать версию ResNet18 для разрешения 224х224).\n",
    "После предобучения проведите (офлайн) линейный пробинг на всех датасетах (ChestMNIST и PneumoniaMNIST). В отчете продемонстрируйте графики обучения (значение лосса, значение метрик онлайн пробинга в ходе обучения), а также таблицу с финальными результатами. Проанализируйте разницу между SimCLR и VICReg.\n",
    "\n",
    "\n",
    "Итого, краткий план задания:\n",
    "1. Сформируйте собственный набор аугментаций для CXR и добавьте референсный.\n",
    "\n",
    "2. Для каждого набора: предобучение SimCLR 5–10 эпох с онлайн-пробингом; сравниваем метрики.\n",
    "\n",
    "3. Выбираем лучший набор → полное предобучение: SimCLR — 20+ эпох, VICReg — 20+ эпох.\n",
    "\n",
    "4. Выполняем офлайн-линейный пробинг и сравниваем SimCLR и VICReg.\n",
    "\n",
    "**Бонусный балл** получат решения, у которых значения финальных метрик соответсвуют supervised качеству (т.е. как если бы вы обучали ResNet-18 с нуля на каждом датасете). Значения метрик при supervised обучении можно найти [здесь](https://medmnist.com/). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85b5fa8",
   "metadata": {},
   "source": [
    "## Задание 4 (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc2d4d7-8af8-4ef7-ae05-6b6592dd1730",
   "metadata": {},
   "source": [
    "Попробуем начать предобучение не с рандомной инициализации, а с весов, полученных предобучением на естественных картинках. Предлагается два варианта на выбор (надо выбрать один):\n",
    "* веса из библиотеки `torchvision`, которые были получены supervised обучением,\n",
    "* веса из соответствующих чекпоинтов [solo-learn](https://github.com/vturrisi/solo-learn/tree/main), которые были получены self-supervised обучением на Imagenet-100 (100-классовая подвыборка ImageNet'а).\n",
    "\n",
    "Для этого при создании энкодера в `torchvision.models.resnet` можно использовать параметр `weights` у `resnet18()`. \n",
    "После инициализации с предобученных весов, проведите такой же цикл предобучения из предыдущего пункта, и продемонстрируйте разницу в финальном качестве. Помогает или вредит старт с supervised imagenet'овских весов?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SSL",
   "language": "python",
   "name": "ssl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
