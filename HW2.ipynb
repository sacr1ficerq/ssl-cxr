{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dce818ed",
   "metadata": {},
   "source": [
    "# Домашнее задание 2 \n",
    "## Contrastive and non-contrastive methods in CXR images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc72c49b",
   "metadata": {},
   "source": [
    "## Оценивание\n",
    "\n",
    "Задание должно быть выполнено самостоятельно. Похожие решения будут считаться плагиатом. Если вы опирались на внешний источник в реализации, необходимо указать ссылку на него. \n",
    "\n",
    "В качестве решения, необходимо предоставить код (`train.py` с аргументами для выбора датасета/метода) + отчет, в котором будут отображены все детали выбора гиперпараметров, комментарии, сопровождающие графики, а так же ответы на вопросы в ДЗ. Оформляйте отчет четко и читаемо. Плохо оформленный код, плохо оформленные графики негативно скажутся на оценке, так же как и неэффективная реализация."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56f10bb",
   "metadata": {},
   "source": [
    "## Введение\n",
    "\n",
    "Вам предстоит реализовать (задание 0) и поработать с двумя методами - [SimCLR](https://arxiv.org/abs/2002.05709) и [VICReg](https://arxiv.org/abs/2105.04906). Обучать их будем на датасете, относящемся к домену медицинских изображений (задания 1-4). Подключим онлайн пробинг (задание 3), а так же сравним с трансфером с imagenet домена в этот мед домен (задание 4).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e111a04",
   "metadata": {},
   "source": [
    "\n",
    "### Датасеты [MedMNIST+](https://medmnist.com/)\n",
    "Будем использовать уже подготовленные сконвертированные из DICOM'ов картинки. MedMNIST включает в себя два релевантных для нас датасета с рентгеновскими снимками грудной клетки:\n",
    "\n",
    "\n",
    "| MedMNIST2D     | Data Modality | Tasks (# Classes/Labels)           | # Samples | # Training | # Validation | # Test |\n",
    "|----------------|---------------|------------------------------------|-----------|------------|--------------|--------|\n",
    "| ChestMNIST     | Chest X-Ray   | Multi-Label (14), Binary-Class (2) | 112,120   | 78,468     | 11,219       | 22,433 |\n",
    "| PneumoniaMNIST | Chest X-Ray   | Binary-Class (2)                   | 5,856     | 4,708      | 524          | 624    |\n",
    "\n",
    "На этот раз будем использовать разрешение 224x224 (необходимо выставить `size` при инициализации датасета). Несколько картинок из ChestMNIST:\n",
    "\n",
    "![CXR image examples from ChestMNIST](data/cxr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808d9f49",
   "metadata": {},
   "source": [
    "## Задание 0 \n",
    "(2 балла)\n",
    "\n",
    "Реализуйте SimCLR и VICReg на базе ResNet-18 энкодера. Для этого надо реализовать соответствующие лосс-функции и архитектуры проекционных голов. Убедитесь, в корректности реализации на CIFAR-10 (не забудьте применить коррекцию резнета для картинок разрешением 32x32 из предыдущего домашнего задания). Для этого, сначала сделайте предобучение на train части датасета в течении 100 эпох, затем сделайте линейный пробинг с замороженным выучившимся энкодером.\n",
    "\n",
    "Референсный интервал top-1 accuracy для 100 эпох предобучения ~80-83% на линейном пробинге (если не получается, проверьте реализацию оптимизатора (**LARS**) и расписания шага обучения (`warmup_cosine`) или попробуйте подвигать learning rate).\n",
    "\n",
    "**NB**\n",
    "Чтобы сэкономить на психотерапевте, используйте оптимизатор [LARS](https://arxiv.org/abs/1708.03888) и `LinearWarmupCosineAnnealing` шедулер. Их нет в торче, но довольно просто реализовать самому или взять референсную реализацию."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8803efb6",
   "metadata": {},
   "source": [
    "## Задание 1\n",
    "(1 балл)\n",
    "\n",
    "Загрузите упомянутые датасеты из `MedMNIST+` и проанализируйте данные. Например, посмотрите на количество и баланс классов, как устроена разметка по классам, найдите среднее и дисперсию значений пикселей. Определите **подходящие метрики и лосс** для конечной задачи для **каждого** из датасетов, аргументировано объясните ваш выбор."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaa5120",
   "metadata": {},
   "source": [
    "## Задание 2\n",
    "(2 балла)\n",
    "\n",
    "CXR изображения выглядят специфично. Кажется, что нужно иметь и специфичные для таких картинок аугментации.\n",
    "Поиграйтесь с трансформами и зафиксируйте набор, с которым вы будете проводить финальные запуски предобучения.\n",
    "\n",
    "### Каким образом можно определить подходящие аугментации?\n",
    "\n",
    "За неимением экспертного знания (если есть знакомый врач-рентгенолог, можно посоветоваться), будем отталкиваться от набора аугментаций в естественных картинках. Начнем с набора, используемого в SimCLR-подобных методах, для ImageNet. Примерно так готовый набор выглядит в `torchvision`'е (обратите внимание, что при создании `СolorJitter` указываются не сами интвервалы, а дельта, т.е. `brightness=0.4` дает `(0.6, 1.4)`):\n",
    "\n",
    "```python\n",
    "Compose(\n",
    "      RandomResizedCrop(\n",
    "          size=(224, 224),\n",
    "          scale=(0.08, 1.0),\n",
    "          ratio=(0.75, 1.3333333333333333),\n",
    "          interpolation=InterpolationMode.BICUBIC,\n",
    "          antialias=True)\n",
    "      RandomApply(\n",
    "          ColorJitter(\n",
    "              brightness=(0.6, 1.4),\n",
    "              contrast=(0.6, 1.4),\n",
    "              saturation=(0.8, 1.2),\n",
    "              hue=(-0.1, 0.1)))\n",
    "      RandomGrayscale(p=0.2)\n",
    "      GaussianBlur(p=0.5)\n",
    "      Solarization(p=0.1)\n",
    "      RandomHorizontalFlip(p=0.5)\n",
    "      ToTensor()\n",
    "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.228, 0.224, 0.225], inplace=False)\n",
    "```\n",
    "\n",
    "Какие параметры аугментаций стоило бы поменять? Реализуйте набор трансформов, посмотрите какие картинки получаются на выходе (не забудьте перевести выход в нужный интервал значений для визуализации), поиграйтесь со значениями параметров (например, `scale` в `RandomResizedCrop` или `brightness` в `ColorJitter`). Какие трансформы стоит убрать? Попробуйте добавить инвертирование и повороты картинок на небольшой угол."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7ed726",
   "metadata": {},
   "source": [
    "## Задание 3\n",
    "(4 балла)\n",
    "\n",
    "Для того, чтобы честно найти подходящий набор аугментаций, надо проводить этап предобучения и затем оценивать качество получившихся репрезентаций на конечной задаче. Можно ли это как-то ускорить? Раз у нас есть разметка для всего датасета, воспользуемся ей и ускорим подбор аугментаций с помощью online probing'a.\n",
    "Для этого, добавим голову для линейного пробинга `linear_probe` к нашему энкодеру (`backbone`) и проекционной голове (`projection_head`). Эта линейная \"проба\" будет состоять из одного линейного слоя из размерности выхода энкодера (например, 512 для ResNet18) в число классов на конечной задаче (например, 14 классов у ChestMNIST).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881f35e8",
   "metadata": {},
   "source": [
    "На каждой итерации **предобучения** будем учить `backbone` и `projection_head` на претекстовую задачу (например, SimCLR лосс), а линейную пробу на классификацию.\n",
    "Получается такая двуглавая архитектура, где градиенты с претекстового лосса текут по проекционной голове и энкодеру, а градиенты с классификационного лосса только по линейной пробе (не забудьте правильно `detach'`нуться).\n",
    "\n",
    "```\n",
    "                      projection_head(h) -> ssl_loss\n",
    "                    /\n",
    "x -> encoder(x) -> h\n",
    "                    \\\n",
    "                      probe(h.detach()) -> cls_loss\n",
    "```\n",
    "\n",
    "Записать это можно примерно так:\n",
    "```python\n",
    "for batch in aug_dataloader:\n",
    "  x1, x2, y = batch\n",
    "  h1, h2 = model.backbone(x1, x2)\n",
    "  z1, z2 = model.projection_head(h1, h2)\n",
    "  logits = model.linear_probe(h1.detach())\n",
    "  total_loss = ssl_loss(z1, z2) + cls_loss(yhat, y)\n",
    "  total_loss.backward()\n",
    "```\n",
    "\n",
    "Таким образом, мы сможем в реальном времени наблюдать за тем, как обучение на претекстовую задачу влияет на качество репрезентаций для конечной задачи. Конечно, это не то же самое, что провести полный цикл предобучения, а затем измерить качество на конечной задаче. Тем не менее, это обеспечивает быструю итерацию по конфигурациям гиперпараметров (например, выбор аугментаций). Можно делать запуски на небольшое число эпох и сравнивать онлайн метрики.\n",
    "\n",
    "*NB* Если вспомнить STL-10 из ДЗ 1, разметка была доступна только для небольшого подмножества (`train` vs `unlabeled`). В таком случае онлайн пробинг все еще можно делать, пробу можно обучать во время валидационной эпохи на размеченном сплите (веса энкодера заморожены).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f6f8c4",
   "metadata": {},
   "source": [
    "### Этап отбора аугментаций\n",
    "Добавьте онлайн пробинг в пайплайн обучения SimCLR. Воспользуемся результатами онлайн пробинга на ранних эпохах для отбора аугментаций. Предложите свой набор аугментаций исходя из общих соображений и анализа из **задания 2**, так же можно попробовать [RandAugment](https://docs.pytorch.org/vision/main/generated/torchvision.transforms.RandAugment.html). В качестве референсного набора зафиксируем следующую композицию трансформов:\n",
    "\n",
    "```python\n",
    "Compose(\n",
    "      RandomApply(    \n",
    "          RandomRotation(degrees=[-10.0, 10.0],\n",
    "          interpolation=InterpolationMode.NEAREST,\n",
    "          expand=False,\n",
    "          fill=0))\n",
    "      RandomResizedCrop(\n",
    "          size=(224, 224),\n",
    "          scale=(0.5, 1.0),\n",
    "          ratio=(0.75, 1.3333333333333333),\n",
    "          interpolation=InterpolationMode.BICUBIC, antialias=True)\n",
    "      RandomApply(    \n",
    "          ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2)))\n",
    "      RandomHorizontalFlip(p=0.5)\n",
    "      RandomApply(    \n",
    "          Lambda(<lambda>, types=['object']))\n",
    "      ToTensor()\n",
    "      Normalize(mean=[0.5], std=[0.5], inplace=False)\n",
    "),\n",
    "```\n",
    "где `<lambda>` это функция для инвертирования изображения.\n",
    "Сравните выбранный вами набор аугментаций и референсный, какой из них лучше? Для сравнения можете ориентироваться на метрики онлайн пробинга на 5-10 эпохах.\n",
    "\n",
    "### Этап полного предобучения\n",
    "Зафиксируйте \"лучший\" набор и выполните полное предобучение (например, 50 эпох) с методами реализованными **задания 0**: SimCLR и VICReg (не забудьте использовать версию ResNet18 для разрешения 224х224).\n",
    "После предобучения проведите (офлайн) линейный пробинг на всех датасетах (ChestMNIST и PneumoniaMNIST). В отчете продемонстрируйте графики обучения (значение лосса, значение метрик онлайн пробинга в ходе обучения), а также таблицу с финальными результатами. Проанализируйте разницу между SimCLR и VICReg.\n",
    "\n",
    "\n",
    "Итого, краткий план задания:\n",
    "1. Сформируйте собственный набор аугментаций для CXR и добавьте референсный.\n",
    "\n",
    "2. Для каждого набора: предобучение SimCLR 5–10 эпох с онлайн-пробингом; сравниваем метрики.\n",
    "\n",
    "3. Выбираем лучший набор → полное предобучение: SimCLR — 20+ эпох, VICReg — 20+ эпох.\n",
    "\n",
    "4. Выполняем офлайн-линейный пробинг и сравниваем SimCLR и VICReg.\n",
    "\n",
    "**Бонусный балл** получат решения, у которых значения финальных метрик соответсвуют supervised качеству (т.е. как если бы вы обучали ResNet-18 с нуля на каждом датасете). Значения метрик при supervised обучении можно найти [здесь](https://medmnist.com/). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85b5fa8",
   "metadata": {},
   "source": [
    "## Задание 4\n",
    "(1 балл)\n",
    "\n",
    "Попробуем начать предобучение не с рандомной инициализации, а с весов, полученных предобучением на естественных картинках. Предлагается два варианта на выбор (надо выбрать один):\n",
    "* веса из библиотеки `torchvision`, которые были получены supervised обучением,\n",
    "* веса из соответствующих чекпоинтов [solo-learn](https://github.com/vturrisi/solo-learn/tree/main), которые были получены self-supervised обучением на Imagenet-100 (100-классовая подвыборка ImageNet'а).\n",
    "\n",
    "Для этого при создании энкодера в `torchvision.models.resnet` можно использовать параметр `weights` у `resnet18()`. \n",
    "После инициализации с предобученных весов, проведите такой же цикл предобучения из предыдущего пункта, и продемонстрируйте разницу в финальном качестве. Помогает или вредит старт с supervised imagenet'овских весов?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ead04fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
